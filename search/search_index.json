{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About this repository This repo is the canonical source for Kubernetes Operators that appear on OperatorHub.io , OpenShift Container Platform and OKD . Documentation contributions For changes in the documentation, please raise a PR against redhat-openshift-ecosystem/community-operators-pipeline:documentation . Add your Operator We would love to see your Operator added to this collection. We currently use automated vetting via continuous integration plus manual review to curate a list of high-quality, well-documented Operators. If you are new to Kubernetes Operators start here . If you have an existing Operator read our contribution guidelines on how to package and test it. Then test your Operator locally and submit a Pull Request. Test your Operator before submitting a PR IMPORTANT Kubernetes has been deprecating API(s), which will be removed and no longer available in 1.22 and in the Openshift version 4.9 . Note that your project will be unable to use them on OCP 4.9/K8s 1.22 . Then, it is strongly recommended to check Deprecated API Migration Guide from v1.22 and ensure that your projects have them migrated and are not using any deprecated API. If you are looking to distribute your solution on OKD/Openshift catalogs see OKD/OpenShift Catalogs criteria and options . You can use our test suite to test your Operator before submitting it. Our test suite will help you to install it. Then assuming you followed the contribution guide, you can run the entire suite on a Linux or macOS system with Docker installed: cd <community-operators-project> bash < ( curl -sL https://raw.githubusercontent.com/redhat-openshift-ecosystem/community-operators-pipeline/ci/latest/ci/scripts/opp.sh ) \\ kiwi,lemon,orange \\ <operator-stream>/<operator-name>/<operator-version> Tests are not passing or do you want to know more? Check test suite for more info. Validating the bundle with SDK Note that you can validate the bundle via [ operator-sdk bundle validate ][sdk-cli-bundle-validate] against the entire suite of validators for Operator Framework, in addition to the required bundle validators: operator-sdk bundle validate ./bundle --select-optional suite = operatorframework Note If you used operator-sdk to build your project and to build your bundle with the target make bundle then you can leverage [ operator-sdk scorecard ][sdk-cli-scorecard-bundle] to perform functional tests as well. Preview your Operator on OperatorHub.io You can preview how your Operator would be rendered there by using the preview tool . If you are submitting your Operator in the upstream-community-operators directory your Operator will appear on OperatorHub.io. Submitting your PR Review this checklist upon creating a PR and after you acknowledged the contribution guidelines. Do not forget to add ci.yaml to the top level of your operator. Otherwise only semver mode will be supported. Update your Operator Similarly, to update your operator you need to submit a PR with any changes to your Operator resources. Refer to our contribution guide for more details. CI Tests your Operator Upon creating a pull request against this repo, a set of CI pipelines will run, see more details here . The pipeline will actually run the same commands you use to test locally. You can help speed up the review of your PR by testing locally, either manually or using scripts . For troubleshooting failing tests consult the manual test steps or see specific error messages solved in troubleshooting guide . Reporting Bugs Use the issue tracker in this repository to report bugs.","title":"Overview"},{"location":"#about-this-repository","text":"This repo is the canonical source for Kubernetes Operators that appear on OperatorHub.io , OpenShift Container Platform and OKD .","title":"About this repository"},{"location":"#documentation-contributions","text":"For changes in the documentation, please raise a PR against redhat-openshift-ecosystem/community-operators-pipeline:documentation .","title":"Documentation contributions"},{"location":"#add-your-operator","text":"We would love to see your Operator added to this collection. We currently use automated vetting via continuous integration plus manual review to curate a list of high-quality, well-documented Operators. If you are new to Kubernetes Operators start here . If you have an existing Operator read our contribution guidelines on how to package and test it. Then test your Operator locally and submit a Pull Request.","title":"Add your Operator"},{"location":"#test-your-operator-before-submitting-a-pr","text":"IMPORTANT Kubernetes has been deprecating API(s), which will be removed and no longer available in 1.22 and in the Openshift version 4.9 . Note that your project will be unable to use them on OCP 4.9/K8s 1.22 . Then, it is strongly recommended to check Deprecated API Migration Guide from v1.22 and ensure that your projects have them migrated and are not using any deprecated API. If you are looking to distribute your solution on OKD/Openshift catalogs see OKD/OpenShift Catalogs criteria and options . You can use our test suite to test your Operator before submitting it. Our test suite will help you to install it. Then assuming you followed the contribution guide, you can run the entire suite on a Linux or macOS system with Docker installed: cd <community-operators-project> bash < ( curl -sL https://raw.githubusercontent.com/redhat-openshift-ecosystem/community-operators-pipeline/ci/latest/ci/scripts/opp.sh ) \\ kiwi,lemon,orange \\ <operator-stream>/<operator-name>/<operator-version> Tests are not passing or do you want to know more? Check test suite for more info.","title":"Test your Operator before submitting a PR"},{"location":"#validating-the-bundle-with-sdk","text":"Note that you can validate the bundle via [ operator-sdk bundle validate ][sdk-cli-bundle-validate] against the entire suite of validators for Operator Framework, in addition to the required bundle validators: operator-sdk bundle validate ./bundle --select-optional suite = operatorframework Note If you used operator-sdk to build your project and to build your bundle with the target make bundle then you can leverage [ operator-sdk scorecard ][sdk-cli-scorecard-bundle] to perform functional tests as well.","title":"Validating the bundle with SDK"},{"location":"#preview-your-operator-on-operatorhubio","text":"You can preview how your Operator would be rendered there by using the preview tool . If you are submitting your Operator in the upstream-community-operators directory your Operator will appear on OperatorHub.io.","title":"Preview your Operator on OperatorHub.io"},{"location":"#submitting-your-pr","text":"Review this checklist upon creating a PR and after you acknowledged the contribution guidelines. Do not forget to add ci.yaml to the top level of your operator. Otherwise only semver mode will be supported.","title":"Submitting your PR"},{"location":"#update-your-operator","text":"Similarly, to update your operator you need to submit a PR with any changes to your Operator resources. Refer to our contribution guide for more details.","title":"Update your Operator"},{"location":"#ci-tests-your-operator","text":"Upon creating a pull request against this repo, a set of CI pipelines will run, see more details here . The pipeline will actually run the same commands you use to test locally. You can help speed up the review of your PR by testing locally, either manually or using scripts . For troubleshooting failing tests consult the manual test steps or see specific error messages solved in troubleshooting guide .","title":"CI Tests your Operator"},{"location":"#reporting-bugs","text":"Use the issue tracker in this repository to report bugs.","title":"Reporting Bugs"},{"location":"action/","text":"Community operator action V1 This action runs community operator tests. What's new Supported tests (kiwi, lemon, orange) Own community-operators fork and branch supported Run test from own repository. Doesn't have to be community-operators . More info in op-action-examples Usage - uses : operator-framework/community-operators@v1 with : # Test type (kiwi,lemon or orange) test-type : 'kiwi' # Operator stream name (community-operators or upstream-community-operators) stream : 'community-operators' # Operator name (exmaple 'aqua') name : '' # Operator version (exmaple '5.3.0') version : '' # Community operators repo # Default: 'https://github.com/operator-framework/community-operators.git' repo : '' # Community operators branch # Default: 'master' branch : '' # Repo directory when if not community-operators # Default: 'community-operators' repo-dir : '' # Space separated list of labels in PR # Default: '' pr-labels : '' # Path to operator version content (for example local/path/to/operator/version). # Default: '' operator-version-path : '' # Path to package file (for example local/path/to/my-operator.package.yaml). # Default: '' package-path : '' # Path to ci.yaml file (for example local/path/to/ci.yaml). # Default: '' ci-path : '' Test 'kiwi' aqua operator version 5.3.0 for community-operators - uses : operator-framework/community-operators@v1 with : test-type : 'kiwi' stream : 'upstream-community-operators' name : 'aqua' version : '5.3.0' Test 'kiwi' aqua operator version 5.3.0 for upstream-community-operators - uses : operator-framework/community-operators@v1 with : test-type : 'kiwi' stream : 'upstream-community-operators' name : 'aqua' version : '5.3.0' Test 'lemon' aqua operator version 5.3.0 for upstream-community-operators - uses : operator-framework/community-operators@v1 with : test-type : 'lemon' stream : 'upstream-community-operators' name : 'aqua' version : '5.3.0' Test 'orange' (catalog v4.6) for aqua operator version 5.3.0 for community-operators - uses : operator-framework/community-operators@v1 with : test-type : 'oragne_v4.6' stream : 'community-operators' name : 'aqua' version : '5.3.0' Test 'kiwi' aqua operator version 5.3.0 for upstream-community-operators in the own project Test a single version of an operator from a custom project. Follwoing will happen: Action will clone https://github.com/operator-framework/community-operators.git in to master branch (controlled by repo: and branch: ) Enters directory community-operators (controlled by repo-dir: ) Removes directory upstream-community-operators/aqua/5.3.0 Creates directory upstream-community-operators/aqua/5.3.0 Copy content my/op/manifest to upstream-community-operators/aqua/5.3.0 (controlled by operator-version-path: ) Copy/overwrite my/op/aqua-operator.package.yaml to upstream-community-operators/aqua/ (controlled by package-path: ) Copy/overwrite my/op/ci.yaml to upstream-community-operators/aqua/ (controlled by ci-path: ) Runs kiwi test (controlled by test-type: ) - uses : operator-framework/community-operators@v1 with : test-type : 'kiwi' stream : 'upstream-community-operators' name : 'aqua' version : '5.3.0' repo : 'https://github.com/operator-framework/community-operators.git' branch : 'master' operator-version-path : my/op/manifest package-path : my/op/aqua-operator.package.yaml ci-path : my/op/ci.yaml License The scripts and documentation in this project are released under the MIT License","title":"Github Action"},{"location":"action/#community-operator-action-v1","text":"This action runs community operator tests.","title":"Community operator action V1"},{"location":"action/#whats-new","text":"Supported tests (kiwi, lemon, orange) Own community-operators fork and branch supported Run test from own repository. Doesn't have to be community-operators . More info in op-action-examples","title":"What's new"},{"location":"action/#usage","text":"- uses : operator-framework/community-operators@v1 with : # Test type (kiwi,lemon or orange) test-type : 'kiwi' # Operator stream name (community-operators or upstream-community-operators) stream : 'community-operators' # Operator name (exmaple 'aqua') name : '' # Operator version (exmaple '5.3.0') version : '' # Community operators repo # Default: 'https://github.com/operator-framework/community-operators.git' repo : '' # Community operators branch # Default: 'master' branch : '' # Repo directory when if not community-operators # Default: 'community-operators' repo-dir : '' # Space separated list of labels in PR # Default: '' pr-labels : '' # Path to operator version content (for example local/path/to/operator/version). # Default: '' operator-version-path : '' # Path to package file (for example local/path/to/my-operator.package.yaml). # Default: '' package-path : '' # Path to ci.yaml file (for example local/path/to/ci.yaml). # Default: '' ci-path : ''","title":"Usage"},{"location":"action/#test-kiwi-aqua-operator-version-530-for-community-operators","text":"- uses : operator-framework/community-operators@v1 with : test-type : 'kiwi' stream : 'upstream-community-operators' name : 'aqua' version : '5.3.0'","title":"Test 'kiwi' aqua operator version 5.3.0 for community-operators"},{"location":"action/#test-kiwi-aqua-operator-version-530-for-upstream-community-operators","text":"- uses : operator-framework/community-operators@v1 with : test-type : 'kiwi' stream : 'upstream-community-operators' name : 'aqua' version : '5.3.0'","title":"Test 'kiwi' aqua operator version 5.3.0 for upstream-community-operators"},{"location":"action/#test-lemon-aqua-operator-version-530-for-upstream-community-operators","text":"- uses : operator-framework/community-operators@v1 with : test-type : 'lemon' stream : 'upstream-community-operators' name : 'aqua' version : '5.3.0'","title":"Test 'lemon' aqua operator version 5.3.0 for upstream-community-operators"},{"location":"action/#test-orange-catalog-v46-for-aqua-operator-version-530-for-community-operators","text":"- uses : operator-framework/community-operators@v1 with : test-type : 'oragne_v4.6' stream : 'community-operators' name : 'aqua' version : '5.3.0'","title":"Test 'orange' (catalog v4.6) for aqua operator version 5.3.0 for community-operators"},{"location":"action/#test-kiwi-aqua-operator-version-530-for-upstream-community-operators-in-the-own-project","text":"Test a single version of an operator from a custom project. Follwoing will happen: Action will clone https://github.com/operator-framework/community-operators.git in to master branch (controlled by repo: and branch: ) Enters directory community-operators (controlled by repo-dir: ) Removes directory upstream-community-operators/aqua/5.3.0 Creates directory upstream-community-operators/aqua/5.3.0 Copy content my/op/manifest to upstream-community-operators/aqua/5.3.0 (controlled by operator-version-path: ) Copy/overwrite my/op/aqua-operator.package.yaml to upstream-community-operators/aqua/ (controlled by package-path: ) Copy/overwrite my/op/ci.yaml to upstream-community-operators/aqua/ (controlled by ci-path: ) Runs kiwi test (controlled by test-type: ) - uses : operator-framework/community-operators@v1 with : test-type : 'kiwi' stream : 'upstream-community-operators' name : 'aqua' version : '5.3.0' repo : 'https://github.com/operator-framework/community-operators.git' branch : 'master' operator-version-path : my/op/manifest package-path : my/op/aqua-operator.package.yaml ci-path : my/op/ci.yaml","title":"Test 'kiwi' aqua operator version 5.3.0 for upstream-community-operators in the own project"},{"location":"action/#license","text":"The scripts and documentation in this project are released under the MIT License","title":"License"},{"location":"best-practices/","text":"Operator Best Practices Check the sections Best Practices for OLM and SDK projects to know more about its best practices and common recommendations, suggestions and conventions, see: SDK best practices OLM best practices","title":"Best practices"},{"location":"best-practices/#operator-best-practices","text":"Check the sections Best Practices for OLM and SDK projects to know more about its best practices and common recommendations, suggestions and conventions, see: SDK best practices OLM best practices","title":"Operator Best Practices"},{"location":"contributing-prerequisites/","text":"Before submitting your Operator First off, thanks for taking the time to contribute your Operator! A primer to Kubernetes Community Operators This project collects Operators and publishes them to OperatorHub.io, a central registry for community-powered Kubernetes Operators. For OperatorHub.io your Operator needs to work with vanilla Kubernetes. This project also collects Community Operators that work with OpenShift to be displayed in the embedded OperatorHub. If you are new to Operators, start here . Sign Your Work The contribution process works off standard git Pull Requests . Every PR needs to be signed. The sign-off is a simple line at the end of the explanation for a commit. Your signature certifies that you wrote the patch or otherwise have the right to contribute the material. The rules are pretty simple if you can certify the below (from developercertificate.org ): Developer Certificate of Origin Version 1.1 Copyright (C) 2004, 2006 The Linux Foundation and its contributors. 1 Letterman Drive Suite D4700 San Francisco, CA, 94129 Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. Developer's Certificate of Origin 1.1 By making a contribution to this project, I certify that: (a) The contribution was created in whole or in part by me and I have the right to submit it under the open source license indicated in the file; or (b) The contribution is based upon previous work that, to the best of my knowledge, is covered under an appropriate open source license and I have the right under that license to submit that work with modifications, whether created in whole or in part by me, under the same open source license (unless I am permitted to submit under a different license), as indicated in the file; or (c) The contribution was provided directly to me by some other person who certified (a), (b) or (c) and I have not modified it. (d) I understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information I submit with it, including my sign-off) is maintained indefinitely and may be redistributed consistent with this project or the open source license(s) involved. Then you just add a line to every git commit message: Signed - off - by : John Doe < john . doe @example . com > Use your real name (sorry, no pseudonyms or anonymous contributions.) If you set your user.name and user.email git configs, you can sign your commit automatically with git commit -s . Note: If your git config information is set properly then viewing the git log information for your commit will look something like this: Author: John Doe <john.doe@example.com> Date: Mon Oct 21 12:23:17 2019 -0800 Update README Signed-off-by: John Doe <john.doe@example.com> Notice the Author and Signed-off-by lines must match .","title":"Prerequisites"},{"location":"contributing-prerequisites/#before-submitting-your-operator","text":"First off, thanks for taking the time to contribute your Operator!","title":"Before submitting your Operator"},{"location":"contributing-prerequisites/#a-primer-to-kubernetes-community-operators","text":"This project collects Operators and publishes them to OperatorHub.io, a central registry for community-powered Kubernetes Operators. For OperatorHub.io your Operator needs to work with vanilla Kubernetes. This project also collects Community Operators that work with OpenShift to be displayed in the embedded OperatorHub. If you are new to Operators, start here .","title":"A primer to Kubernetes Community Operators"},{"location":"contributing-prerequisites/#sign-your-work","text":"The contribution process works off standard git Pull Requests . Every PR needs to be signed. The sign-off is a simple line at the end of the explanation for a commit. Your signature certifies that you wrote the patch or otherwise have the right to contribute the material. The rules are pretty simple if you can certify the below (from developercertificate.org ): Developer Certificate of Origin Version 1.1 Copyright (C) 2004, 2006 The Linux Foundation and its contributors. 1 Letterman Drive Suite D4700 San Francisco, CA, 94129 Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. Developer's Certificate of Origin 1.1 By making a contribution to this project, I certify that: (a) The contribution was created in whole or in part by me and I have the right to submit it under the open source license indicated in the file; or (b) The contribution is based upon previous work that, to the best of my knowledge, is covered under an appropriate open source license and I have the right under that license to submit that work with modifications, whether created in whole or in part by me, under the same open source license (unless I am permitted to submit under a different license), as indicated in the file; or (c) The contribution was provided directly to me by some other person who certified (a), (b) or (c) and I have not modified it. (d) I understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information I submit with it, including my sign-off) is maintained indefinitely and may be redistributed consistent with this project or the open source license(s) involved. Then you just add a line to every git commit message: Signed - off - by : John Doe < john . doe @example . com > Use your real name (sorry, no pseudonyms or anonymous contributions.) If you set your user.name and user.email git configs, you can sign your commit automatically with git commit -s . Note: If your git config information is set properly then viewing the git log information for your commit will look something like this: Author: John Doe <john.doe@example.com> Date: Mon Oct 21 12:23:17 2019 -0800 Update README Signed-off-by: John Doe <john.doe@example.com> Notice the Author and Signed-off-by lines must match .","title":"Sign Your Work"},{"location":"contributing-via-pr/","text":"Submitting your Operator via Pull Requests (PR) in community operators project Overview To submit an operator one has to do these steps Test your operator locally Fork project https://github.com/operator-framework/community-operators Make a pull request Place the operator in the target directory. More info community-operators (Openshift operator) upstream-community-operators (Kubernetes operator) Configure ci.yaml file. More info Setup reviewers Operator versioning strategy Verify tests and fix problems, if possible Ask for help in the PR in case of problems Test locally before you contribute The team behind OperatorHub.io will support you in making sure your Operator works and is packaged correctly. You can accelerate your submission greatly by testing your Operator with the Operator Framework by following our documentation for local manual testing or automated testing using scripts . You are responsible for testing your Operator's APIs when deployed with OLM. Pull request When a pull request is created, a number of tests are executed. One can see the results in Checks tab. Verify CI test results Every PR against this repository is tested via Continuous Integration . During these tests your Operator will be deployed on either a minikube or OpenShift 4 environments and checked for a healthy deployment. Also several tools are run to check your bundle for completeness. These are the same tools as referenced in our testing docs and testing scripts . Pay attention to the result of GitHub checks. More detailed information about our Continuous Integration process can be found here Test results Test results are located in Checks tab. Then they can be found in Operator test list on the left side. Once clicked on it the summary of the test will be shown. There are multiple tests. For easy mapping, different fruit names were chosen. Look at our testing suite for more information. One can see more details about tests when clicking directly on them. Test on an Openshift cluster For an Openshift operator (operators in community-operators directory), the deployment of an operator is executed on an Openshift cluster via ci/prow/deploy-operator-on-openshift . Note The kiwi test does not include the deployment test on k8s cluster. This can be forced by specifying label test/force-deploy-on-kubernetes in the PR. You are done User is done when all tests are green. When the PR is merged, one can follow the process explained in Release pipeline . Test results failed? When operator tests are failing, one can see a following picture In case of failures, please have a look at the logs of specific tests. If an error is not clear to you, please ask in the PR. Maintainers will be happy to help you with it. Useful commands interacting with the pipeline You can post the following comment/command: Command Functionality /test deploy-operator-on-openshift Restarts Openshift deploy (Operator installation on an Openshift test) /hold Setting PR on hold to prevent merging /hold cancel Tests will be triggered again and merged if possible /retest Tests will be retriggered again (in Openshift case all failed prow jobs are restarted)","title":"Creating pull request (PR)"},{"location":"contributing-via-pr/#submitting-your-operator-via-pull-requests-pr-in-community-operators-project","text":"","title":"Submitting your Operator via Pull Requests (PR) in community operators project"},{"location":"contributing-via-pr/#overview","text":"To submit an operator one has to do these steps Test your operator locally Fork project https://github.com/operator-framework/community-operators Make a pull request Place the operator in the target directory. More info community-operators (Openshift operator) upstream-community-operators (Kubernetes operator) Configure ci.yaml file. More info Setup reviewers Operator versioning strategy Verify tests and fix problems, if possible Ask for help in the PR in case of problems","title":"Overview"},{"location":"contributing-via-pr/#test-locally-before-you-contribute","text":"The team behind OperatorHub.io will support you in making sure your Operator works and is packaged correctly. You can accelerate your submission greatly by testing your Operator with the Operator Framework by following our documentation for local manual testing or automated testing using scripts . You are responsible for testing your Operator's APIs when deployed with OLM.","title":"Test locally before you contribute"},{"location":"contributing-via-pr/#pull-request","text":"When a pull request is created, a number of tests are executed. One can see the results in Checks tab.","title":"Pull request"},{"location":"contributing-via-pr/#verify-ci-test-results","text":"Every PR against this repository is tested via Continuous Integration . During these tests your Operator will be deployed on either a minikube or OpenShift 4 environments and checked for a healthy deployment. Also several tools are run to check your bundle for completeness. These are the same tools as referenced in our testing docs and testing scripts . Pay attention to the result of GitHub checks. More detailed information about our Continuous Integration process can be found here","title":"Verify CI test results"},{"location":"contributing-via-pr/#test-results","text":"Test results are located in Checks tab. Then they can be found in Operator test list on the left side. Once clicked on it the summary of the test will be shown. There are multiple tests. For easy mapping, different fruit names were chosen. Look at our testing suite for more information. One can see more details about tests when clicking directly on them.","title":"Test results"},{"location":"contributing-via-pr/#test-on-an-openshift-cluster","text":"For an Openshift operator (operators in community-operators directory), the deployment of an operator is executed on an Openshift cluster via ci/prow/deploy-operator-on-openshift . Note The kiwi test does not include the deployment test on k8s cluster. This can be forced by specifying label test/force-deploy-on-kubernetes in the PR.","title":"Test on an Openshift cluster"},{"location":"contributing-via-pr/#you-are-done","text":"User is done when all tests are green. When the PR is merged, one can follow the process explained in Release pipeline .","title":"You are done"},{"location":"contributing-via-pr/#test-results-failed","text":"When operator tests are failing, one can see a following picture In case of failures, please have a look at the logs of specific tests. If an error is not clear to you, please ask in the PR. Maintainers will be happy to help you with it.","title":"Test results failed?"},{"location":"contributing-via-pr/#useful-commands-interacting-with-the-pipeline","text":"You can post the following comment/command: Command Functionality /test deploy-operator-on-openshift Restarts Openshift deploy (Operator installation on an Openshift test) /hold Setting PR on hold to prevent merging /hold cancel Tests will be triggered again and merged if possible /retest Tests will be retriggered again (in Openshift case all failed prow jobs are restarted)","title":"Useful commands interacting with the pipeline"},{"location":"contributing-where-to/","text":"Where to contribute There are 2 directories where you can contribute, depending on some basic requirements and where you would like your Operator to show up: Target Directory Appears on Target Platform Requirements community-operators Embedded OperatorHub in OpenShift 4 OpenShift / OKD needs to work on OpenShift 4 or newer upstream-community-operators OperatorHub.io Upstream Kubernetes needs to work on Kubernetes 1.7 or newer These repositories are used by OpenShift 4 and OperatorHub.io respectively. Specifically, Operators for Upstream Kubernetes for OperatorHub.io won't automatically appear on the embedded OperatorHub in OpenShift and should not be used on OpenShift. If your Operator works on both Kubernetes and OpenShift, place a copy of your packaged bundle in the upstream-community-operators directory, as well as the community-operators directory. Submit them as separate PRs. For partners and ISVs, certified operators can now be submitted via connect.redhat.com. If you have submitted your Operator there already, please ensure your submission here uses a different package name (refer to the README for more details).","title":"Where to place operator"},{"location":"contributing-where-to/#where-to-contribute","text":"There are 2 directories where you can contribute, depending on some basic requirements and where you would like your Operator to show up: Target Directory Appears on Target Platform Requirements community-operators Embedded OperatorHub in OpenShift 4 OpenShift / OKD needs to work on OpenShift 4 or newer upstream-community-operators OperatorHub.io Upstream Kubernetes needs to work on Kubernetes 1.7 or newer These repositories are used by OpenShift 4 and OperatorHub.io respectively. Specifically, Operators for Upstream Kubernetes for OperatorHub.io won't automatically appear on the embedded OperatorHub in OpenShift and should not be used on OpenShift. If your Operator works on both Kubernetes and OpenShift, place a copy of your packaged bundle in the upstream-community-operators directory, as well as the community-operators directory. Submit them as separate PRs. For partners and ISVs, certified operators can now be submitted via connect.redhat.com. If you have submitted your Operator there already, please ensure your submission here uses a different package name (refer to the README for more details).","title":"Where to contribute"},{"location":"maintainer-documentation/","text":"Testing pipeline maintainer documentation This documentation is focused on OCP operators. A maintainer is responsible for PR review on the following link https://github.com/redhat-openshift-ecosystem/community-operators-prod/pulls The first part is related to the testing pipeline and how to get a PR green and merged. For failed operator release handling, please jump to Release pipeline maintainer documentation . Overview When a pull request (PR) is opened, tests are automatically triggered to ensure that it meets all quality standards. Tests produce labels. If the PR passes these tests, it is automatically merged, and the new operator is published to a specific index. Do use the merge button. It can cause a failed release. Always make PR green and it will be merged automatically. To restart a test use /retest comment or as a last resort close and reopen a PR. For an automatic merge to be executed non of *-failed labels, but the following three labels must be present: graph LR id7([package-validated]) --> id1(automatc merge is triggered) id10([installation-validated]) --> id1(automatc merge is triggered) id11([authorized-changes]) --> id1(automatc merge is triggered) We will go over situations when some label is missing or something went wrong in the following paragraphs. The package-validated label is missing graph LR id7(Operator test / kiwi / Full operator test) --> id1([package-validated]) id10(Operator test / lemon / Deploy from scratch) --> id1([package-validated]) id11(Operator test / orange / Deploy o7t) --> id1([package-validated]) To ensure that a release pipeline will not fail, a simulated local release is triggered. It is executing the same steps as sharp lease, the only difference is that everything is happening locally and not pushing to any official registry. When a simulated release to an index was successful, the label package-validated is applied. There are three tests as described below. Orange test Simulating a release of an affected operator to the current index(es). Helpful to prevent future failures on current indexes. Lemon test It catches incompatibilities in a release of a new index from scratch. Orange, lemon and kiwi have to obtain package-validated label. Kiwi test Basic checks like linting. The installation-validated label is missing This means that the pipeline can install the operator. For OCP, prow jobs for every supported OCP are triggered. Prow job For every index we support, a dedicated cluster is started and we are testing if operator installation without any problems. If the operator will not be released to some cluster, an installation test is not needed. In this case, a specific installation test passes early without any installation attempt. To debug a red prow job go to Details -> Artifacts -> artifacts/deploy-operator-on-openshift/deploy-operator/build-log.txt When all supported OCP clusters are green a label installation-validated is applied. graph LR ci/prow/4.8-deploy-operator-on-openshift --> id123([installation-started-4.8]) --> id23([installation-validated-4.8]) ci/prow/4.9-deploy-operator-on-openshift --> id133([installation-started-4.9]) --> id24([installation-validated-4.9]) ci/prow/4.10-deploy-operator-on-openshift --> id143([installation-started-4.10]) --> id25([installation-validated-4.10]) ci/prow/4.11-deploy-operator-on-openshift --> id153([installation-started-4.11]) --> id26([installation-validated-4.11]) ci/prow/4.12-deploy-operator-on-openshift --> id163([installation-started-4.12]) --> id27([installation-validated-4.12]) id123([installation-started-4.8]) --> id28([installation-failed-4.8]) id133([installation-started-4.9]) --> id29([installation-failed-4.9]) id143([installation-started-4.10]) --> id30([installation-failed-4.10]) id153([installation-started-4.11]) --> id31([installation-failed-4.11]) id163([installation-started-4.12]) --> id32([installation-failed-4.12]) id23([installation-validated-4.8]) --> id33([installation-validated]) id24([installation-validated-4.9]) --> id33([installation-validated]) id25([installation-validated-4.10]) --> id33([installation-validated]) id26([installation-validated-4.11]) --> id33([installation-validated]) id27([installation-validated-4.12]) --> id33([installation-validated]) The label openshift-started-<VERSION> is present after a couple of hours In case of openshift-started-<VERSION> label is present and prow job is not running, it means that the temporary index was triggered but not finished and failed due to some reason. You can inspect it on Actions -> Prepare Test Index . The authorized-changes label is missing graph LR id1(Operator CI Labels / authorized-changes-handler) --> id2([authorized-changes]) There are a few reasons why the authorized-changes label may be missing from a PR: New-operator label is present If the new-operator label is present, the following steps should be taken: Copy the contents of the clusterserviceversion.yaml file to https://operatorhub.io/preview Visually inspect the content to ensure that it looks correct and that all fields on the right do not contain N/A , except for the channel field. The channel field cannot display any information. No reviewer in ci.yaml file If the authorized-changes label is missing and the ci.yaml file does not include a reviewer, the following step should be taken: Apply the authorized-changes label to the PR. An author is not in the reviewer list in ci.yaml and ci.yaml file was not modified If the @contributor_name please approve message is displayed, indicating that the author of the PR is not in the reviewer list in the ci.yaml file and the ci.yaml file has not been modified, the following steps should be taken: Wait for approval from a reviewer who is listed in the ci.yaml file. If the authorized-changes label is not set after the approval, follow one of the three options documented here to set it. An author is not in the reviewer list in ci.yaml and ci.yaml file is modified If the /hold Please note that ci.yaml was changed message is displayed, indicating that the author of the PR is not in the reviewer list in the ci.yaml file and the ci.yaml file has been modified, the following steps should be taken: Wait for approval from a reviewer who is listed in the ci.yaml file. If necessary, add the /unhold command to the PR to trigger the worklfow. If the authorized-changes label is not set after approval, follow one of the three options documented here . Changes to an existing operator In an ideal world, a contributor is opening a new PR with a new operator version every time. However, in reality, the contributor needs to update an existing operator in rare cases. A current pipeline allows it because it always removes an existing package from an index and also creates a new bundle with the same tag if needed. In general, it is not recommended to overwrite an existing operator version. But if there is some typo in the description or something cosmetic. The current setup allows cosmetic changes defined in local.yml as dc_changes_allowed variable. Pipeline detects a cosmetic change and removes the whole package from an index, then adds the package in the current state containing already changed (new) bundles. In some cases, a contributor may have a strong reason to make more significant changes to an existing operator. When a maintainer decides that this reason is valid an exception can be made. Applying allow/serious-changes-to-existing label will not fail on noncosmetic change then. DCO failed The pipeline is checking if every commit is signed. This is an easy fix, just follow the steps under Details belonging to DCO test. Everything is green but not merged There can be a case when do-not-merge/hold label is present. If the openshift-robot has added it, please remove it. If a contributor has added it, ask for removal. Release pipeline maintainer documentation","title":"Maintainer"},{"location":"maintainer-documentation/#testing-pipeline-maintainer-documentation","text":"This documentation is focused on OCP operators. A maintainer is responsible for PR review on the following link https://github.com/redhat-openshift-ecosystem/community-operators-prod/pulls The first part is related to the testing pipeline and how to get a PR green and merged. For failed operator release handling, please jump to Release pipeline maintainer documentation .","title":"Testing pipeline maintainer documentation"},{"location":"maintainer-documentation/#overview","text":"When a pull request (PR) is opened, tests are automatically triggered to ensure that it meets all quality standards. Tests produce labels. If the PR passes these tests, it is automatically merged, and the new operator is published to a specific index. Do use the merge button. It can cause a failed release. Always make PR green and it will be merged automatically. To restart a test use /retest comment or as a last resort close and reopen a PR. For an automatic merge to be executed non of *-failed labels, but the following three labels must be present: graph LR id7([package-validated]) --> id1(automatc merge is triggered) id10([installation-validated]) --> id1(automatc merge is triggered) id11([authorized-changes]) --> id1(automatc merge is triggered) We will go over situations when some label is missing or something went wrong in the following paragraphs.","title":"Overview"},{"location":"maintainer-documentation/#the-package-validated-label-is-missing","text":"graph LR id7(Operator test / kiwi / Full operator test) --> id1([package-validated]) id10(Operator test / lemon / Deploy from scratch) --> id1([package-validated]) id11(Operator test / orange / Deploy o7t) --> id1([package-validated]) To ensure that a release pipeline will not fail, a simulated local release is triggered. It is executing the same steps as sharp lease, the only difference is that everything is happening locally and not pushing to any official registry. When a simulated release to an index was successful, the label package-validated is applied. There are three tests as described below.","title":"The package-validated label is missing"},{"location":"maintainer-documentation/#the-installation-validated-label-is-missing","text":"This means that the pipeline can install the operator. For OCP, prow jobs for every supported OCP are triggered.","title":"The installation-validated label is missing"},{"location":"maintainer-documentation/#the-authorized-changes-label-is-missing","text":"graph LR id1(Operator CI Labels / authorized-changes-handler) --> id2([authorized-changes]) There are a few reasons why the authorized-changes label may be missing from a PR:","title":"The authorized-changes label is missing"},{"location":"maintainer-documentation/#changes-to-an-existing-operator","text":"In an ideal world, a contributor is opening a new PR with a new operator version every time. However, in reality, the contributor needs to update an existing operator in rare cases. A current pipeline allows it because it always removes an existing package from an index and also creates a new bundle with the same tag if needed. In general, it is not recommended to overwrite an existing operator version. But if there is some typo in the description or something cosmetic. The current setup allows cosmetic changes defined in local.yml as dc_changes_allowed variable. Pipeline detects a cosmetic change and removes the whole package from an index, then adds the package in the current state containing already changed (new) bundles. In some cases, a contributor may have a strong reason to make more significant changes to an existing operator. When a maintainer decides that this reason is valid an exception can be made. Applying allow/serious-changes-to-existing label will not fail on noncosmetic change then.","title":"Changes to an existing operator"},{"location":"maintainer-documentation/#dco-failed","text":"The pipeline is checking if every commit is signed. This is an easy fix, just follow the steps under Details belonging to DCO test.","title":"DCO failed"},{"location":"maintainer-documentation/#everything-is-green-but-not-merged","text":"There can be a case when do-not-merge/hold label is present. If the openshift-robot has added it, please remove it. If a contributor has added it, ask for removal.","title":"Everything is green but not merged"},{"location":"maintainer-documentation/#release-pipeline-maintainer-documentation","text":"","title":"Release pipeline maintainer documentation"},{"location":"operator-ci-yaml/","text":"Operator Publishing / Review settings Each operator might have ci.yaml configuration file to be present in an operator directory (for example community-operators/aqua/ci.yaml ). This configuration file is used by community-operators pipeline to setup various features like reviewers or operator versioning . Note One can create or modify ci.yaml file with a new operator version. This operation can be done in the same PR with other operator changes. Reviewers If you want to accelerate publishing your changes, consider adding yourself and others you trust to the reviewers list. If the author of PR will be in that list, changes she/he made will be taken as authorized changes (label authorized-changes will be set). This will be the indicator for our pipeline that the PR is ready to merge automatically. Note If an author of PR is not in reviewers list, PR will not be merged automatically. For automatic merging, PR needs to have authorized-changes label set. Manually or by pipeline finding a reviewer in ci.yaml on main branch. For security reasons, we are not checking reviewers from a fork. Note If an auhor of PR is not in reviewers list and reviewers are present in ci.yaml file. All reviewers will be mentioned in PR comment to check for upcoming changes. For this to work, it is required to setup reviewers in ci.yaml file. It can be done by adding reviewers tag with a list of gGitHub usernames. For example Example $ cat <path-to-operator>/ci.yaml --- reviewers: - user1 - user2 Operator versioning Operators have multiple versions. When a new version is released, OLM can update an operator automatically. There are 2 update strategies possible, which are defined in ci.yaml at the operator top level. replaces-mode Every next version defines which version will be replaced using replaces key in the CSV file. It means, that there is a possibility to omit some versions from the update graph . The best practice is to put them in a separate channel then. semver-mode Every version will be replaced by the next higher version according to semantic versioning. Restrictions A contributor can decide if semver-mode or replaces-mode mode will be used for a specific operator. By default, replaces-mode is activated, when ci.yaml file is present and contains updateGraph: replaces-mode . When a contributor decides to switch and use semver-mode , it will be specified in ci.yaml file or the key updateGraph will be missing. Example $ cat <path-to-operator>/ci.yaml --- # Use `replaces-mode` or `semver-mode`. updateGraph: replaces-mode Automatic Cluster Version Label (OCP) - packagemanifest only Starting OCP v4.9 (based on k8s 1.22) some old APIs were deprecated ( Deprecated API Migration Guide from v1.22 , OKD/OpenShift Catalogs criteria and options ). Users can set com.redhat.openshift.versions: <versions> in its bundle annotations.yaml file to limit specific operator versions to be visible on a certain cluster. The following example will install the operator bundle for versions v4.6, v4.7, v4.8 $ cat <path-to-operators>/<name>/<version/metadata>/annotations.yaml annotations: com.redhat.openshift.versions: \"v4.6-v4.8\" For packagemanifest format, it is not possible and community-operators pipeline can automatically set a such label to the bundle. Users have to allow it by putting packagemanifestClusterVersionLabel: auto in ci.yaml file $ cat <path-to-operator>/ci.yaml packagemanifestClusterVersionLabel: auto Kubernetes max version in CSV Starting from kubernetes 1.22 some old APIs were deprecated ( Deprecated API Migration Guide from v1.22 . Users can set operatorhub.io/ui-metadata-max-k8s-version: \"<version>\" in its CSV file to inform its maximum supported Kubernetes version. The following example will inform that operator can handle 1.21 as maximum Kubernetes version $ cat <path-to-operators>/<name>/<version>/.../my.clusterserviceversion.yaml metadata: annotations: operatorhub.io/ui-metadata-max-k8s-version: \"1.21\"","title":"Operator Publishing / Review settings"},{"location":"operator-ci-yaml/#operator-publishing-review-settings","text":"Each operator might have ci.yaml configuration file to be present in an operator directory (for example community-operators/aqua/ci.yaml ). This configuration file is used by community-operators pipeline to setup various features like reviewers or operator versioning . Note One can create or modify ci.yaml file with a new operator version. This operation can be done in the same PR with other operator changes.","title":"Operator Publishing / Review settings"},{"location":"operator-ci-yaml/#reviewers","text":"If you want to accelerate publishing your changes, consider adding yourself and others you trust to the reviewers list. If the author of PR will be in that list, changes she/he made will be taken as authorized changes (label authorized-changes will be set). This will be the indicator for our pipeline that the PR is ready to merge automatically. Note If an author of PR is not in reviewers list, PR will not be merged automatically. For automatic merging, PR needs to have authorized-changes label set. Manually or by pipeline finding a reviewer in ci.yaml on main branch. For security reasons, we are not checking reviewers from a fork. Note If an auhor of PR is not in reviewers list and reviewers are present in ci.yaml file. All reviewers will be mentioned in PR comment to check for upcoming changes. For this to work, it is required to setup reviewers in ci.yaml file. It can be done by adding reviewers tag with a list of gGitHub usernames. For example","title":"Reviewers"},{"location":"operator-ci-yaml/#example","text":"$ cat <path-to-operator>/ci.yaml --- reviewers: - user1 - user2","title":"Example"},{"location":"operator-ci-yaml/#operator-versioning","text":"Operators have multiple versions. When a new version is released, OLM can update an operator automatically. There are 2 update strategies possible, which are defined in ci.yaml at the operator top level.","title":"Operator versioning"},{"location":"operator-ci-yaml/#replaces-mode","text":"Every next version defines which version will be replaced using replaces key in the CSV file. It means, that there is a possibility to omit some versions from the update graph . The best practice is to put them in a separate channel then.","title":"replaces-mode"},{"location":"operator-ci-yaml/#semver-mode","text":"Every version will be replaced by the next higher version according to semantic versioning.","title":"semver-mode"},{"location":"operator-ci-yaml/#restrictions","text":"A contributor can decide if semver-mode or replaces-mode mode will be used for a specific operator. By default, replaces-mode is activated, when ci.yaml file is present and contains updateGraph: replaces-mode . When a contributor decides to switch and use semver-mode , it will be specified in ci.yaml file or the key updateGraph will be missing.","title":"Restrictions"},{"location":"operator-ci-yaml/#example_1","text":"$ cat <path-to-operator>/ci.yaml --- # Use `replaces-mode` or `semver-mode`. updateGraph: replaces-mode","title":"Example"},{"location":"operator-ci-yaml/#automatic-cluster-version-label-ocp-packagemanifest-only","text":"Starting OCP v4.9 (based on k8s 1.22) some old APIs were deprecated ( Deprecated API Migration Guide from v1.22 , OKD/OpenShift Catalogs criteria and options ). Users can set com.redhat.openshift.versions: <versions> in its bundle annotations.yaml file to limit specific operator versions to be visible on a certain cluster. The following example will install the operator bundle for versions v4.6, v4.7, v4.8 $ cat <path-to-operators>/<name>/<version/metadata>/annotations.yaml annotations: com.redhat.openshift.versions: \"v4.6-v4.8\" For packagemanifest format, it is not possible and community-operators pipeline can automatically set a such label to the bundle. Users have to allow it by putting packagemanifestClusterVersionLabel: auto in ci.yaml file $ cat <path-to-operator>/ci.yaml packagemanifestClusterVersionLabel: auto","title":"Automatic Cluster Version Label (OCP) - packagemanifest only"},{"location":"operator-ci-yaml/#kubernetes-max-version-in-csv","text":"Starting from kubernetes 1.22 some old APIs were deprecated ( Deprecated API Migration Guide from v1.22 . Users can set operatorhub.io/ui-metadata-max-k8s-version: \"<version>\" in its CSV file to inform its maximum supported Kubernetes version. The following example will inform that operator can handle 1.21 as maximum Kubernetes version $ cat <path-to-operators>/<name>/<version>/.../my.clusterserviceversion.yaml metadata: annotations: operatorhub.io/ui-metadata-max-k8s-version: \"1.21\"","title":"Kubernetes max version in CSV"},{"location":"operator-overwrite/","text":"Orange test fails even when my operator is ok. There is one case when your operator is correct, but orange test might fail. This happened when some operator versions are already published and one wants to change some cosmetic changes to bundle or convert format from package manifest to bundle format. In these scenarios, one can follow the instruction bellow Operator version overwrite Operator recreate Operator version overwrite When cosmetic changes are made to an already published operator version Orange test will fail. In this case, one needs to have allow/operator-version-overwrite label set. One can set it or ask a maintainer to set it for you. After the PR will be merged, the following changes will happen The bundle for a current operator version will be overwritten Build catalog with a new bundle Operator recreate When a whole operator is recreated (usually when converting a whole operator from packagemanifest format to bundle format). One needs to have allow/operator-recreate label set. One can set it or ask a maintainer to set it for you. After the PR will be merged, the following changes will happen Delete operator Rebuild all bundles Build the catalog with new bundles","title":"Updating published Operator version"},{"location":"operator-overwrite/#orange-test-fails-even-when-my-operator-is-ok","text":"There is one case when your operator is correct, but orange test might fail. This happened when some operator versions are already published and one wants to change some cosmetic changes to bundle or convert format from package manifest to bundle format. In these scenarios, one can follow the instruction bellow Operator version overwrite Operator recreate","title":"Orange test fails even when my operator is ok."},{"location":"operator-overwrite/#operator-version-overwrite","text":"When cosmetic changes are made to an already published operator version Orange test will fail. In this case, one needs to have allow/operator-version-overwrite label set. One can set it or ask a maintainer to set it for you. After the PR will be merged, the following changes will happen The bundle for a current operator version will be overwritten Build catalog with a new bundle","title":"Operator version overwrite"},{"location":"operator-overwrite/#operator-recreate","text":"When a whole operator is recreated (usually when converting a whole operator from packagemanifest format to bundle format). One needs to have allow/operator-recreate label set. One can set it or ask a maintainer to set it for you. After the PR will be merged, the following changes will happen Delete operator Rebuild all bundles Build the catalog with new bundles","title":"Operator recreate"},{"location":"operator-release-process/","text":"Operator release Operator release workflow Release workflow contains all jobs. This can be found in Actions tab of the project. When an operator is merged to the master following scenarios will happen: For a k8s case (upstream-community-operators) Push to quay (to support old app registry) Build index image for k8s Build image for operatorhub.io page Deploy operatorhub.io page For an Openshift case (community-operators) Push to quay (to support old app registry) Build index image for different Openshift versions (v4.6 and v4.7 in this case) and multiarch image is also produced. Build image for dev.operatorhub.io page (for development purposes only) Deploy dev.operatorhub.io page (for development purposes only) Operator is published After this process, your operator will be published. Index image location For Openshift: registry.redhat.io/redhat/community-operator-index:v4.8 registry.redhat.io/redhat/community-operator-index:v4.9 registry.redhat.io/redhat/community-operator-index:v4.10 registry.redhat.io/redhat/community-operator-index:v4.11 registry.redhat.io/redhat/community-operator-index:v4.12 Note registry.redhat.io/redhat/community-operator-index:latest - this is a clone of v4.6 from historical reasons as it always was a clone of v4.6 . Will be deprecated in the future. For Kubernetes: quay.io/operatorhubio/catalog:latest Bundle images location For Openshift: quay.io/openshift-community-operators/ For Kubernetes: quay.io/operatorhubio/","title":"Release process"},{"location":"operator-release-process/#operator-release","text":"","title":"Operator release"},{"location":"operator-release-process/#operator-release-workflow","text":"Release workflow contains all jobs. This can be found in Actions tab of the project. When an operator is merged to the master following scenarios will happen:","title":"Operator release workflow"},{"location":"operator-release-process/#for-a-k8s-case-upstream-community-operators","text":"Push to quay (to support old app registry) Build index image for k8s Build image for operatorhub.io page Deploy operatorhub.io page","title":"For a k8s case (upstream-community-operators)"},{"location":"operator-release-process/#for-an-openshift-case-community-operators","text":"Push to quay (to support old app registry) Build index image for different Openshift versions (v4.6 and v4.7 in this case) and multiarch image is also produced. Build image for dev.operatorhub.io page (for development purposes only) Deploy dev.operatorhub.io page (for development purposes only)","title":"For an Openshift case (community-operators)"},{"location":"operator-release-process/#operator-is-published","text":"After this process, your operator will be published.","title":"Operator is published"},{"location":"operator-release-process/#index-image-location","text":"For Openshift: registry.redhat.io/redhat/community-operator-index:v4.8 registry.redhat.io/redhat/community-operator-index:v4.9 registry.redhat.io/redhat/community-operator-index:v4.10 registry.redhat.io/redhat/community-operator-index:v4.11 registry.redhat.io/redhat/community-operator-index:v4.12 Note registry.redhat.io/redhat/community-operator-index:latest - this is a clone of v4.6 from historical reasons as it always was a clone of v4.6 . Will be deprecated in the future. For Kubernetes: quay.io/operatorhubio/catalog:latest","title":"Index image location"},{"location":"operator-release-process/#bundle-images-location","text":"For Openshift: quay.io/openshift-community-operators/ For Kubernetes: quay.io/operatorhubio/","title":"Bundle images location"},{"location":"operator-test-suite/","text":"Operator tests Running tests Run tests by entering 'community-operators' project directory and run the following command using the options bellow. ' ' and ' ' options are optional. cd <community-operators> OPP_PRODUCTION_TYPE=<k8s/ocp> \\ OPP_DEBUG= 1 \\ OPP_AUTO_PACKAGEMANIFEST_CLUSTER_VERSION_LABEL=1 \\ OPP_RELEASE_INDEX_NAME=\"catalog_tmp\" \\ bash <(curl -sL https://raw.githubusercontent.com/redhat-openshift-ecosystem/community-operators-pipeline/ci/latest/ci/scripts/opp.sh) \\ <test-type1,test-type2,...,test-typeN> \\ <operator-version-dir-relative-to-community-operators-project> \\ [<git repo>] [<git branch>] Test type The list of tests is shown in the following table : Test type Description kiwi Full operator test lemon Full test of operator to be deployed from scratch orange Full test of operator to be deployed with existing bundles in quay registry all kiwi,lemon,orange Default valus for test By default OPP_PRODUCTION_TYPE=k8s with OPP_K8S_PRODUCTION_VERSION_DEFAULT=latest index version is tested. For Openshift test read bellow. The following example will run the default k8s lemon test that would produce FBC (File Based Index) image with the tag latest bash <(curl -sL https://raw.githubusercontent.com/redhat-openshift-ecosystem/community-operators-pipeline/ci/latest/ci/scripts/opp.sh) \\ lemon \\ operators/aqua/1.0.2 OCP test and index versions Note With variable OPP_PRODUCTION_TYPE=ocp Openshift test will be run and v4.9-db will be as a default version. This can be controlled by a variable OPP_OCP_PRODUCTION_VERSION_DEFAULT=v4.9-db or by adding it to a test name orange_v4.9-db . More info about versions is bellow Index Description v4.9 v4.9 in FBC(File Based Catalog) format v4.9-db v4.9 in old sql format The following example will run a default ocp lemon test that would produce DB (old DB index format ) image with tag v4.9 OPP_PRODUCTION_TYPE=ocp OPP_OCP_PRODUCTION_VERSION_DEFAULT=v4.9-db \\ bash <(curl -sL https://raw.githubusercontent.com/redhat-openshift-ecosystem/community-operators-pipeline/ci/latest/ci/scripts/opp.sh) \\ lemon \\ operators/aqua/1.0.2 one can use the following as an equivalent (to omit OPP_OCP_PRODUCTION_VERSION_DEFAULT variable) OPP_PRODUCTION_TYPE=ocp \\ bash <(curl -sL https://raw.githubusercontent.com/redhat-openshift-ecosystem/community-operators-pipeline/ci/latest/ci/scripts/opp.sh) \\ lemon_v4.9-db\\ operators/aqua/1.0.2 Logs Logs can be found in /tmp/op-test/log.out Testing log files If an operator test fails, one can enter the testing container via the following command. One can substitute 'docker' with 'podman' when supported docker exec -it op-test /bin/bash Examples Running tests from a local directory The following example will run 'all' tests on 'aqua' operator with version '1.0.2' from 'operators (k8s)' directory. 'community-operators' project will be taken from a local directory one is running the command from ($PWD). cd <community-operators> bash <(curl -sL https://raw.githubusercontent.com/redhat-openshift-ecosystem/community-operators-pipeline/ci/latest/ci/scripts/opp.sh) \\ all \\ operators/aqua/1.0.2 Running tests from official 'community-operators' repo The following example will run 'kiwi' and 'lemon' tests on 'aqua' operator with version '1.0.2' from 'community-operators (Openshift)' directory. 'community-operators' project will be taken from the git repo 'https://github.com/operator-framework/community-operators' and 'master' branch cd <community-operators> bash <(curl -sL https://raw.githubusercontent.com/redhat-openshift-ecosystem/community-operators-pipeline/ci/latest/ci/scripts/opp.sh) \\ kiwi,lemon \\ community-operators/aqua/1.0.2 \\ https://github.com/operator-framework/community-operators \\ master Running tests from forked 'community-operators' repo and specific branch The following example will run 'kiwi' and 'lemon' tests on 'kong' operator with version '0.5.0' from 'operators (k8s)' directory.'community-operators' project will be taken from the git repo 'https://github.com/Kong/community-operators' and 'release/v0.5.0' branch ('https://github.com/Kong/community-operators/tree/release/v0.5.0') cd <community-operators> bash <(curl -sL https://raw.githubusercontent.com/redhat-openshift-ecosystem/community-operators-pipeline/ci/latest/ci/scripts/opp.sh) \\ kiwi,lemon \\ operators/kong/0.5.0 \\ https://github.com/Kong/community-operators \\ release/v0.5.0 Misc Name Description Default OP_TEST_DEBUG Debug level (0-3) 0 OP_TEST_CONTAINER_TOOL Container tool used on host docker OP_TEST_DRY_RUN Will print commands to be executed 0 Testing operators by Ansible Documentation for testing is located here","title":"Via test suite"},{"location":"operator-test-suite/#operator-tests","text":"","title":"Operator tests"},{"location":"operator-test-suite/#running-tests","text":"Run tests by entering 'community-operators' project directory and run the following command using the options bellow. ' ' and ' ' options are optional. cd <community-operators> OPP_PRODUCTION_TYPE=<k8s/ocp> \\ OPP_DEBUG= 1 \\ OPP_AUTO_PACKAGEMANIFEST_CLUSTER_VERSION_LABEL=1 \\ OPP_RELEASE_INDEX_NAME=\"catalog_tmp\" \\ bash <(curl -sL https://raw.githubusercontent.com/redhat-openshift-ecosystem/community-operators-pipeline/ci/latest/ci/scripts/opp.sh) \\ <test-type1,test-type2,...,test-typeN> \\ <operator-version-dir-relative-to-community-operators-project> \\ [<git repo>] [<git branch>]","title":"Running tests"},{"location":"operator-test-suite/#test-type","text":"The list of tests is shown in the following table : Test type Description kiwi Full operator test lemon Full test of operator to be deployed from scratch orange Full test of operator to be deployed with existing bundles in quay registry all kiwi,lemon,orange","title":"Test type"},{"location":"operator-test-suite/#default-valus-for-test","text":"By default OPP_PRODUCTION_TYPE=k8s with OPP_K8S_PRODUCTION_VERSION_DEFAULT=latest index version is tested. For Openshift test read bellow. The following example will run the default k8s lemon test that would produce FBC (File Based Index) image with the tag latest bash <(curl -sL https://raw.githubusercontent.com/redhat-openshift-ecosystem/community-operators-pipeline/ci/latest/ci/scripts/opp.sh) \\ lemon \\ operators/aqua/1.0.2","title":"Default valus for test"},{"location":"operator-test-suite/#ocp-test-and-index-versions","text":"Note With variable OPP_PRODUCTION_TYPE=ocp Openshift test will be run and v4.9-db will be as a default version. This can be controlled by a variable OPP_OCP_PRODUCTION_VERSION_DEFAULT=v4.9-db or by adding it to a test name orange_v4.9-db . More info about versions is bellow Index Description v4.9 v4.9 in FBC(File Based Catalog) format v4.9-db v4.9 in old sql format The following example will run a default ocp lemon test that would produce DB (old DB index format ) image with tag v4.9 OPP_PRODUCTION_TYPE=ocp OPP_OCP_PRODUCTION_VERSION_DEFAULT=v4.9-db \\ bash <(curl -sL https://raw.githubusercontent.com/redhat-openshift-ecosystem/community-operators-pipeline/ci/latest/ci/scripts/opp.sh) \\ lemon \\ operators/aqua/1.0.2 one can use the following as an equivalent (to omit OPP_OCP_PRODUCTION_VERSION_DEFAULT variable) OPP_PRODUCTION_TYPE=ocp \\ bash <(curl -sL https://raw.githubusercontent.com/redhat-openshift-ecosystem/community-operators-pipeline/ci/latest/ci/scripts/opp.sh) \\ lemon_v4.9-db\\ operators/aqua/1.0.2","title":"OCP test and index versions"},{"location":"operator-test-suite/#logs","text":"Logs can be found in /tmp/op-test/log.out","title":"Logs"},{"location":"operator-test-suite/#testing-log-files","text":"If an operator test fails, one can enter the testing container via the following command. One can substitute 'docker' with 'podman' when supported docker exec -it op-test /bin/bash","title":"Testing log files"},{"location":"operator-test-suite/#examples","text":"","title":"Examples"},{"location":"operator-test-suite/#running-tests-from-a-local-directory","text":"The following example will run 'all' tests on 'aqua' operator with version '1.0.2' from 'operators (k8s)' directory. 'community-operators' project will be taken from a local directory one is running the command from ($PWD). cd <community-operators> bash <(curl -sL https://raw.githubusercontent.com/redhat-openshift-ecosystem/community-operators-pipeline/ci/latest/ci/scripts/opp.sh) \\ all \\ operators/aqua/1.0.2","title":"Running tests from a local directory"},{"location":"operator-test-suite/#running-tests-from-official-community-operators-repo","text":"The following example will run 'kiwi' and 'lemon' tests on 'aqua' operator with version '1.0.2' from 'community-operators (Openshift)' directory. 'community-operators' project will be taken from the git repo 'https://github.com/operator-framework/community-operators' and 'master' branch cd <community-operators> bash <(curl -sL https://raw.githubusercontent.com/redhat-openshift-ecosystem/community-operators-pipeline/ci/latest/ci/scripts/opp.sh) \\ kiwi,lemon \\ community-operators/aqua/1.0.2 \\ https://github.com/operator-framework/community-operators \\ master","title":"Running tests from official 'community-operators' repo"},{"location":"operator-test-suite/#running-tests-from-forked-community-operators-repo-and-specific-branch","text":"The following example will run 'kiwi' and 'lemon' tests on 'kong' operator with version '0.5.0' from 'operators (k8s)' directory.'community-operators' project will be taken from the git repo 'https://github.com/Kong/community-operators' and 'release/v0.5.0' branch ('https://github.com/Kong/community-operators/tree/release/v0.5.0') cd <community-operators> bash <(curl -sL https://raw.githubusercontent.com/redhat-openshift-ecosystem/community-operators-pipeline/ci/latest/ci/scripts/opp.sh) \\ kiwi,lemon \\ operators/kong/0.5.0 \\ https://github.com/Kong/community-operators \\ release/v0.5.0","title":"Running tests from forked 'community-operators' repo and specific branch"},{"location":"operator-test-suite/#misc","text":"Name Description Default OP_TEST_DEBUG Debug level (0-3) 0 OP_TEST_CONTAINER_TOOL Container tool used on host docker OP_TEST_DRY_RUN Will print commands to be executed 0","title":"Misc"},{"location":"operator-test-suite/#testing-operators-by-ansible","text":"Documentation for testing is located here","title":"Testing operators by Ansible"},{"location":"operator-version-update/","text":"Updating existing Operators Updating operator version Note It is strongly recommended to bump the operator version if possible. Operator version update can be done by creating a new directory with version name in operator dir without ' v '. For example, updating the aqua operator from 1.0.0 to 1.0.1 $ tree community-operators/aqua/ -d community-operators/aqua/ \u251c\u2500\u2500 0.0.1 \u251c\u2500\u2500 0.0.2 \u251c\u2500\u2500 1.0.0 \u251c\u2500\u2500 1.0.1 Minor (cosmetics) changes There is some case when only some minor changes to the existing operator are needed (like a description update or an update of an icon). In this case, the pipeline will set a corresponding label and automatically handle such case. Allowed changes Only changes in an csv (*.clusterserviceversion.yaml) are allowed List of allowed tag changes in csv spec.description spec.DisplayName spec.icon metadata.annotations.description metadata.annotations.alm-examples Operator versioning strategy Sometimes it is needed to change how operator versions are built to the index. This can be controlled by ci.yaml file. More info Reviewers update While an operator is envolving over time, sometimes it is needed to change reviewers. This can be controlled by ci.yaml file. More info","title":"Updating existing Operators"},{"location":"operator-version-update/#updating-existing-operators","text":"","title":"Updating existing Operators"},{"location":"operator-version-update/#updating-operator-version","text":"Note It is strongly recommended to bump the operator version if possible. Operator version update can be done by creating a new directory with version name in operator dir without ' v '. For example, updating the aqua operator from 1.0.0 to 1.0.1 $ tree community-operators/aqua/ -d community-operators/aqua/ \u251c\u2500\u2500 0.0.1 \u251c\u2500\u2500 0.0.2 \u251c\u2500\u2500 1.0.0 \u251c\u2500\u2500 1.0.1","title":"Updating operator version"},{"location":"operator-version-update/#minor-cosmetics-changes","text":"There is some case when only some minor changes to the existing operator are needed (like a description update or an update of an icon). In this case, the pipeline will set a corresponding label and automatically handle such case.","title":"Minor (cosmetics) changes"},{"location":"operator-version-update/#allowed-changes","text":"Only changes in an csv (*.clusterserviceversion.yaml) are allowed List of allowed tag changes in csv spec.description spec.DisplayName spec.icon metadata.annotations.description metadata.annotations.alm-examples","title":"Allowed changes"},{"location":"operator-version-update/#operator-versioning-strategy","text":"Sometimes it is needed to change how operator versions are built to the index. This can be controlled by ci.yaml file. More info","title":"Operator versioning strategy"},{"location":"operator-version-update/#reviewers-update","text":"While an operator is envolving over time, sometimes it is needed to change reviewers. This can be controlled by ci.yaml file. More info","title":"Reviewers update"},{"location":"packaging-operator/","text":"Package your Operator This repository makes use of the Operator Framework and its packaging concept for Operators. Your contribution is welcome in the form of a Pull Request with your Operator packaged for use with Operator Lifecycle Manager . Packaging format Your Operator submission can be formatted following the bundle or packagemanifest format. The packagemanifest format is a legacy format that is kept for backward compatibility only and then, it strongly recommended to use bundle format. The former allows shipping your entire Operator with all its versions in one single directory. The latter allows shipping individual releases in container images. In general, a released version of your Operator is described in a ClusterServiceVersion manifest alongside the CustomResourceDefinitions of your Operator and additional metadata describing your Operator listing. Create a ClusterServiceVersion To add your operator to any of the supported platforms, you will need to submit metadata for your Operator to be used by the Operator Lifecycle Manager (OLM). This is a YAML file called ClusterServiceVersion which contains references to all of the CRDs, RBAC rules, Deployment and container image needed to install and securely run your Operator. It also contains user-visible info like a description of its features and supported Kubernetes versions. Note that your Operator's CRDs are shipped in separate manifests alongside the CSV so OLM can register them during installation (your Operator is not supposed to self-register its CRDs). Follow this guide to create an OLM-compatible CSV for your operator. You can also leverage existing examples in this repository. For more information about the advanced features of the Operator metadata format used here, be sure to check the Operator Lifecycle Manager documentation about how to package webhooks, upgrade readiness probes or an Operator supported on multiple computer architectures (multi-arch). Categories An Operator's CSV must contain the fields mentioned here for it to be displayed properly within the various platforms. If your operator needs a new category, follow the instructions about categories . There is one CSV per version of your Operator alongside the CRDs. Create a release The bundle format has a top-level directory named after your Operator name in the ClusterServiceVersion directory. Inside are sub-directories for the individual bundle, named after the semantic versioning release of your Operator. All metadata is defined within the individual release of the Operator. That is, inside each bundle. This includes the channel definitions. The default channel is also defined within the bundle and overwritten by every new bundle you add (this is a known limitation and is being worked on). Within each version, you have your CustomResourceDefinitions , ClusterServiceVersion file (containing the same name and version of your Operator as defined inside the YAML structure) and some metadata about the bundle. You can learn more about the bundle format here and also see an example . Note that this community project only requires you to submit your bundle in the form of metadata. The integrated release pipeline of this repository will take care of publishing your bundle as a container image and maintaining it in a public catalog. Your directory structure might look like this when using the bundle format. Notice that the Dockerfile is optionally and actually ignored. The processing pipeline of this site builds a container image for each of your bundles regardless. $ tree my-operator/ my-operator/ \u251c\u2500\u2500 0.1.0 \u2502 \u251c\u2500\u2500 manifests \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2502 \u2514\u2500\u2500 my-operator.v0.1.0.clusterserviceversion.yaml \u2502 \u251c\u2500\u2500 metadata \u2502 \u2502 \u2514\u2500\u2500 annotations.yaml \u2502 \u2514\u2500\u2500 Dockerfile \u251c\u2500\u2500 0.5.0 \u2502 \u251c\u2500\u2500 manifests \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2502 \u2514\u2500\u2500 my-operator.v0.5.0.clusterserviceversion.yaml \u2502 \u251c\u2500\u2500 metadata \u2502 \u2502 \u2514\u2500\u2500 annotations.yaml \u2502 \u2514\u2500\u2500 Dockerfile \u251c\u2500\u2500 1.0.0 \u2502 \u251c\u2500\u2500 manifests \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2502 \u2514\u2500\u2500 my-operator.v1.0.0.clusterserviceversion.yaml \u2502 \u251c\u2500\u2500 metadata \u2502 \u2502 \u2514\u2500\u2500 annotations.yaml \u2502 \u2514\u2500\u2500 Dockerfile \u2514\u2500\u2500 2.0.0 \u251c\u2500\u2500 manifests \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v2.0.0.clusterserviceversion.yaml \u251c\u2500\u2500 metadata \u2502 \u2514\u2500\u2500 annotations.yaml \u2514\u2500\u2500 Dockerfile ... If you used operator-sdk to develop your Operator you can also leverage its packaging tooling to create a bundle by just running the target make bundle . Moving from packagemanifest to bundle format Eventually, this repository will only accept bundle format at some point in the future. Also, the bundle format has more features like semver mode or, in the future, installing bundles directly outside of a catalog. Migration of existing content, regardless of whether the Operator was created with the SDK, can be achieved with the opm tool on a per Operator version basis. You can download opm here . Suppose v2.0.0 is the version of the Operator you want to test convert to bundle format directory with the opm tool: mkdir /tmp/my-operator-2.0.0-bundle/ cd /tmp/my-operator-2.0.0-bundle/ opm alpha bundle build --directory /path/to/my-operator/2.0.0-bundle/ --tag my-operator-bundle:v2.0.0 --output-dir . This will have generated the bundle format layout in the current working directory /tmp/my-operator-2.0.0-bundle/ : $ tree . /tmp/my-operator-2.0.0-bundle/ \u251c\u2500\u2500 manifests \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v2.0.0.clusterserviceversion.yaml \u251c\u2500\u2500 metadata \u2502 \u2514\u2500\u2500 annotations.yaml \u2514\u2500\u2500 bundle.Dockerfile You can verify the generated bundle metadata for semantic correctness with the operator-sdk on this directory. operator-sdk bundle validate /tmp/my-operator-2.0.0-bundle/ --select-optional name=operatorhub About the Dockerfile A Dockerfile is typically part of the bundle metadata used to build the bundle image. For security reasons, our release process is generating an internal Dockerfile that is used to build and publish the bundle image. Existing Dockerfile or bundle.Dockerfile will be ignored. You can leverage the annotations.yaml file to control custom labels the resulting image should have. For example: annotations: # Core bundle annotations. operators.operatorframework.io.bundle.mediatype.v1: registry+v1 operators.operatorframework.io.bundle.manifests.v1: manifests/ operators.operatorframework.io.bundle.metadata.v1: metadata/ operators.operatorframework.io.bundle.package.v1: global-load-balancer-operator operators.operatorframework.io.bundle.channels.v1: alpha operators.operatorframework.io.bundle.channel.default.v1: alpha operators.operatorframework.io.metrics.mediatype.v1: metrics+v1 operators.operatorframework.io.metrics.builder: operator-sdk-v1.4.0+git operators.operatorframework.io.metrics.project_layout: go.kubebuilder.io/v3 # Annotations for testing. operators.operatorframework.io.test.mediatype.v1: scorecard+v1 operators.operatorframework.io.test.config.v1: tests/scorecard/ will generate Dockerfile FROM scratch # from metadata/annotations.yaml LABEL operators.operatorframework.io.bundle.mediatype.v1=\"registry+v1\" LABEL operators.operatorframework.io.bundle.manifests.v1=\"manifests/\" LABEL operators.operatorframework.io.bundle.metadata.v1=\"metadata/\" LABEL operators.operatorframework.io.bundle.package.v1=\"global-load-balancer-operator\" LABEL operators.operatorframework.io.bundle.channels.v1=\"alpha\" LABEL operators.operatorframework.io.bundle.channel.default.v1=\"alpha\" LABEL operators.operatorframework.io.metrics.mediatype.v1=\"metrics+v1\" LABEL operators.operatorframework.io.metrics.builder=\"operator-sdk-v1.4.0+git\" LABEL operators.operatorframework.io.metrics.project_layout=\"go.kubebuilder.io/v3\" LABEL operators.operatorframework.io.test.mediatype.v1=\"scorecard+v1\" LABEL operators.operatorframework.io.test.config.v1=\"tests/scorecard/\" COPY ./manifests manifests/ COPY ./metadata metadata/ COPY ./tests/scorecard/ tests/scorecard/ Note If you specify the operators.operatorframework.io.test.config.v1 to embed scorecard tests in your bundle, make sure the supplied directory path (e.g. tests/scorecard/ relative from the bundle root directory) exists, otherwise, the validation will fail. You can download operator-sdk here . Operator icon The icon is defined in a CSV as spec.icon . If you don't have an own icon, you should use a default one: icon: - base64data: \"PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNTguNTEgMjU4LjUxIj48ZGVmcz48c3R5bGU+LmNscy0xe2ZpbGw6I2QxZDFkMTt9LmNscy0ye2ZpbGw6IzhkOGQ4Zjt9PC9zdHlsZT48L2RlZnM+PHRpdGxlPkFzc2V0IDQ8L3RpdGxlPjxnIGlkPSJMYXllcl8yIiBkYXRhLW5hbWU9IkxheWVyIDIiPjxnIGlkPSJMYXllcl8xLTIiIGRhdGEtbmFtZT0iTGF5ZXIgMSI+PHBhdGggY2xhc3M9ImNscy0xIiBkPSJNMTI5LjI1LDIwQTEwOS4xLDEwOS4xLDAsMCwxLDIwNi40LDIwNi40LDEwOS4xLDEwOS4xLDAsMSwxLDUyLjExLDUyLjExLDEwOC40NSwxMDguNDUsMCwwLDEsMTI5LjI1LDIwbTAtMjBoMEM1OC4xNiwwLDAsNTguMTYsMCwxMjkuMjVIMGMwLDcxLjA5LDU4LjE2LDEyOS4yNiwxMjkuMjUsMTI5LjI2aDBjNzEuMDksMCwxMjkuMjYtNTguMTcsMTI5LjI2LTEyOS4yNmgwQzI1OC41MSw1OC4xNiwyMDAuMzQsMCwxMjkuMjUsMFoiLz48cGF0aCBjbGFzcz0iY2xzLTIiIGQ9Ik0xNzcuNTQsMTAzLjQxSDE0MS42NkwxNTQuOSw2NS43NmMxLjI1LTQuNC0yLjMzLTguNzYtNy4yMS04Ljc2SDEwMi45M2E3LjMyLDcuMzIsMCwwLDAtNy40LDZsLTEwLDY5LjYxYy0uNTksNC4xNywyLjg5LDcuODksNy40LDcuODloMzYuOUwxMTUuNTUsMTk3Yy0xLjEyLDQuNDEsMi40OCw4LjU1LDcuMjQsOC41NWE3LjU4LDcuNTgsMCwwLDAsNi40Ny0zLjQ4TDE4NCwxMTMuODVDMTg2Ljg2LDEwOS4yNCwxODMuMjksMTAzLjQxLDE3Ny41NCwxMDMuNDFaIi8+PC9nPjwvZz48L3N2Zz4=\" mediatype: \"image/svg+xml\" Supported formats: svg, jpg, png Updating your existing Operator Unless of a purely cosmetic nature, subsequent updates to your Operator should result in new bundle directories being added, containing an updated CSV as well as copied, updated and/or potentially newly added CRDs. Within your new CSV, update the spec.version field to the desired new semantic version of your Operator. In order to have OLM enable updates to your a new Operator version you can choose between three update modes: semver-mode , semver-skippatch-mode and replaces-mode . The default is semver-mode . If you want to change the default, place a file called ci.yaml in your top-level directory (works for both packagemanifest or bundle format) and set it to either of the two other values. For example: updateGraph : replaces-mode semver-mode OLM treats all your Operator versions with semantic version rules and updates them in order of those versions. That is, every version will be replaced by the next higher version according to a semantic versioning sort order. During an update on the cluster, OLM will update to the latest version, one version at a time. To use this, simply specify spec.version in your CSV. If you accidentally add spec.replaces this will contradict semantic versioning and raise an error. semver-skippatch Works like semver with slightly different behavior of OLM on the cluster, where instead of updating from e.g. 1.1.0 and an update path according to a semver ordering rules like so: 1.1.0 -> 1.1.1 -> 1.1.2 , the update would jump straight to 1.1.2 instead of updating to 1.1.1 first. replaces-mode Each Operator bundle not only contains spec.version but also points to an older version it can upgrade from via spec.replaces key in the CSV file, e.g. replaces: my-operator.v1.0.0 . From this chain of back pointers, OLM computes the update graph at runtime. This allows us to omit some versions from the update graph or release special leaf versions. Regardless of which mode you choose to have OLM create update paths for your Operator, it continuously update your Operator often as new features are added and bugs are fixed. (Legacy) Create a release using the packagemanifest format NOTE It is recommended to use the bundle format instead. This format is still valid for backward compatibility only and at some point will no longer be supported. The packagemanifest format is a directory structure in which the top-level directory represents your Operator as a package . Below that top-level directory is versioned sub-directories, one for each released version of your Operator. The sub-directory names follow a semantic version of your Operator and contain the CustomResourceDefinition s and ClusterServiceVersion . The exact version is the one of your Operator as defined in spec.version inside the CSV. The version should also be reflected in the CSV file name for ease of use. It is required that the spec.name field in the CSV is also the same as the package name. Follow the example below, assuming your Operator package is called my-operator : $ tree my-operator/ my-operator \u251c\u2500\u2500 0 .1.0 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v0.1.0.clusterserviceversion.yaml \u251c\u2500\u2500 0 .5.0 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v0.5.0.clusterserviceversion.yaml \u251c\u2500\u2500 1 .0.0 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v1.0.0.clusterserviceversion.yaml \u2514\u2500\u2500 my-operator.package.yaml The package.yaml is a YAML file at the root level of the package directory. It provides the package name, a selection of channels pointing to potentially different Operator Versions/CSVs and a default channel. The package name is what users on the cluster see when they discover Operators available to install. Channels Use channels to allow your users to select a different update cadence, e.g. stable vs. nightly . If you have only a single channel the use of defaultChannel is optional. An example of my-operator.package.yaml : packageName : my-operator channels : - name : stable currentCSV : my-operator.v1.0.0 - name : nightly currentCSV : my-operator.v1.0.3-beta defaultChannel : stable Your CSV versioning should follow semantic versioning concepts. Again, packageName is the suffix of the package.yaml file name and the field in spec.name in the CSV should all refer to the same Operator name. Operator Bundle Editor You can now create your Operator bundle using the bundle editor . Starting by uploading your Kubernetes YAML manifests, the forms on the page will be populated with all valid information and used to create the new Operator bundle. You can modify or add properties through these forms as well. The result will be a downloadable ZIP file. Provide information about your Operator A large part of the information gathered in the CSV is used for user-friendly visualization on OperatorHub.io or components like the embedded OperatorHub in OpenShift. Your work is on display, so please ensure to provide relevant information in your Operator's description, specifically covering: What the managed application is about and where to find more information The features of your Operator and how to use it Any manual steps required to fulfill pre-requisites for running/installing your Operator","title":"Operator structure"},{"location":"packaging-operator/#package-your-operator","text":"This repository makes use of the Operator Framework and its packaging concept for Operators. Your contribution is welcome in the form of a Pull Request with your Operator packaged for use with Operator Lifecycle Manager .","title":"Package your Operator"},{"location":"packaging-operator/#packaging-format","text":"Your Operator submission can be formatted following the bundle or packagemanifest format. The packagemanifest format is a legacy format that is kept for backward compatibility only and then, it strongly recommended to use bundle format. The former allows shipping your entire Operator with all its versions in one single directory. The latter allows shipping individual releases in container images. In general, a released version of your Operator is described in a ClusterServiceVersion manifest alongside the CustomResourceDefinitions of your Operator and additional metadata describing your Operator listing.","title":"Packaging format"},{"location":"packaging-operator/#updating-your-existing-operator","text":"Unless of a purely cosmetic nature, subsequent updates to your Operator should result in new bundle directories being added, containing an updated CSV as well as copied, updated and/or potentially newly added CRDs. Within your new CSV, update the spec.version field to the desired new semantic version of your Operator. In order to have OLM enable updates to your a new Operator version you can choose between three update modes: semver-mode , semver-skippatch-mode and replaces-mode . The default is semver-mode . If you want to change the default, place a file called ci.yaml in your top-level directory (works for both packagemanifest or bundle format) and set it to either of the two other values. For example: updateGraph : replaces-mode","title":"Updating your existing Operator"},{"location":"packaging-operator/#operator-bundle-editor","text":"You can now create your Operator bundle using the bundle editor . Starting by uploading your Kubernetes YAML manifests, the forms on the page will be populated with all valid information and used to create the new Operator bundle. You can modify or add properties through these forms as well. The result will be a downloadable ZIP file.","title":"Operator Bundle Editor"},{"location":"packaging-operator/#provide-information-about-your-operator","text":"A large part of the information gathered in the CSV is used for user-friendly visualization on OperatorHub.io or components like the embedded OperatorHub in OpenShift. Your work is on display, so please ensure to provide relevant information in your Operator's description, specifically covering: What the managed application is about and where to find more information The features of your Operator and how to use it Any manual steps required to fulfill pre-requisites for running/installing your Operator","title":"Provide information about your Operator"},{"location":"packaging-required-criteria-ocp/","text":"OKD/OpenShift Catalogs criteria and options Overview To distribute on OpenShift Catalogs, you will need to comply with the same standard criteria defined for OperatorHub.io (see Common recommendations and suggestions ). Then, additionally, you have some requirements and options which follow. IMPORTANT Kubernetes has been deprecating API(s) which will be removed and no longer available in 1.22 and in the Openshift version 4.9 . Note that your project will be unable to use them on OCP 4.9/K8s 1.22 and then, it is strongly recommended to check Deprecated API Migration Guide from v1.22 and ensure that your projects have them migrated and are not using any deprecated API. Note that your operator using them will not work in 1.22 and in the Openshift version 4.9 . OpenShift 4.8 introduces two new alerts that fire when an API that will be removed in the next release is in use. Check the event alerts of your Operators running on 4.8 and ensure that you will not find any warning about these API(s) still being used by it. Also, to prevent workflow issues, its users will need to have installed in their OCP cluster a version of your operator compatible with 4.9 before they try to upgrade their cluster from any previous version to 4.9 or higher. In this way, it is recommended to ensure that your operators are no longer using these API(s) versions. However, If you still need to publish the operator bundles with any of these API(s) for use on earlier k8s/OCP versions, ensure that the operator bundle is configured accordingly. Taking the actions below will help prevent users from installing versions of your operator on an incompatible version of OCP, and also prevent them from upgrading to a newer version of OCP that would be incompatible with the version of your operator that is currently installed on their cluster. Configure the max OpenShift Version compatible Use the olm.openShiftMaxVersion annotation in the CSV to prevent the user from upgrading their OCP cluster before upgrading the installed operator version to any distribution which is compatible with: apiVersion : operators.coreos.com/v1alpha1 kind : ClusterServiceVersion metadata : annotations : # Prevent cluster upgrades to OpenShift Version 4.9 when this # bundle is installed on the cluster \"olm.properties\" : '[{\"type\": \"olm.maxOpenShiftVersion\", \"value\": \"4.8\"}]' The CSV annotation will eventually prevent the user from upgrading their OCP cluster before they have installed a version of your operator which is compatible with 4.9 . However, note that it is important to make these changes now as users running workloads with deprecated API(s) that are looking to upgrade to OCP 4.9 will need to be running operators that have this annotation set in order to prevent the cluster upgrade and potentially adversely impacting their crucial workloads. This option is useful when you know that the current version of your project will not work well on some specific Openshift version. Configure the Openshift distribution Use the annotation com.redhat.openshift.versions in bundle/metadata/annotations.yaml to ensure that the index image will be generated with its OCP Label, to prevent the bundle from being distributed on to 4.9: com.redhat.openshift.versions: \"v4.6-v4.8\" This option is also useful when you know that the current version of your project will not work well on some specific OpenShift version. By using it you defined the Openshift versions where the Operator should be distributed and the Operator will not appear in a catalog of an Openshift version that is outside of the range. You must use it if you are distributing a solution that contains deprecated API(s) and will no longer be available in later versions. For more information see Managing OpenShift Versions . Validate the bundle with the common criteria to distribute via OLM with SDK Also, you can check the bundle via operator-sdk bundle validate against the suite Validator Community Operators and the K8s Version that you are intended to publish: operator-sdk bundle validate ./bundle --select-optional suite = operatorframework --optional-values = k8s-version = 1 .22 NOTE: The validators only check the manifests which are shipped in the bundle. They are unable to ensure that the project's code does not use the Deprecated/Removed API(s) in 1.22 and/or that it does not have as dependency another operator that uses them. Validate the bundle with the specific criteria to distribute in Openshift catalogs Pre-requirement Download the binary . You might want to keep it in your $GOPTH/bin Then, we can use the experimental OpenShift OLM Catalog Validator to check your Operator bundle. In this case, we need to inform the bundle and the annotations.yaml file paths: $ ocp-olm-catalog-validator my-bundle-path/bundle --optional-values=\"file=bundle-path/bundle/metadata/annotations.yaml\" Following is an example of an Operator bundle that uses the removed APIs in 1.22 and is not configured accordingly: $ ocp-olm-catalog-validator bundle/ --optional-values = \"file=bundle/metadata/annotations.yaml\" WARN [ 0000 ] Warning: Value memcached-operator.v0.0.1: this bundle is using APIs which were deprecated and removed in v1.22. More info: https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-22. Migrate the API ( s ) for CRD: ([ \"memcacheds.cache.example.com\" ]) ERRO [ 0000 ] Error: Value : ( memcached-operator.v0.0.1 ) olm.maxOpenShiftVersion csv.Annotations not specified with an OCP version lower than 4 .9. This annotation is required to prevent the user from upgrading their OCP cluster before they have installed a version of their operator which is compatible with 4 .9. For further information see https://docs.openshift.com/container-platform/4.8/operators/operator_sdk/osdk-working-bundle-images.html#osdk-control-compat_osdk-working-bundle-images ERRO [ 0000 ] Error: Value : ( memcached-operator.v0.0.1 ) this bundle is using APIs which were deprecated and removed in v1.22. More info: https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-22. Migrate the APIs for this bundle is using APIs which were deprecated and removed in v1.22. More info: https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-22. Migrate the API ( s ) for CRD: ([ \"memcacheds.cache.example.com\" ]) or provide compatible version ( s ) via the labels. ( e.g. LABEL com.redhat.openshift.versions = '4.6-4.8' )","title":"OKD/OpenShift Catalogs criteria and options"},{"location":"packaging-required-criteria-ocp/#okdopenshift-catalogs-criteria-and-options","text":"","title":"OKD/OpenShift Catalogs criteria and options"},{"location":"packaging-required-criteria-ocp/#overview","text":"To distribute on OpenShift Catalogs, you will need to comply with the same standard criteria defined for OperatorHub.io (see Common recommendations and suggestions ). Then, additionally, you have some requirements and options which follow. IMPORTANT Kubernetes has been deprecating API(s) which will be removed and no longer available in 1.22 and in the Openshift version 4.9 . Note that your project will be unable to use them on OCP 4.9/K8s 1.22 and then, it is strongly recommended to check Deprecated API Migration Guide from v1.22 and ensure that your projects have them migrated and are not using any deprecated API. Note that your operator using them will not work in 1.22 and in the Openshift version 4.9 . OpenShift 4.8 introduces two new alerts that fire when an API that will be removed in the next release is in use. Check the event alerts of your Operators running on 4.8 and ensure that you will not find any warning about these API(s) still being used by it. Also, to prevent workflow issues, its users will need to have installed in their OCP cluster a version of your operator compatible with 4.9 before they try to upgrade their cluster from any previous version to 4.9 or higher. In this way, it is recommended to ensure that your operators are no longer using these API(s) versions. However, If you still need to publish the operator bundles with any of these API(s) for use on earlier k8s/OCP versions, ensure that the operator bundle is configured accordingly. Taking the actions below will help prevent users from installing versions of your operator on an incompatible version of OCP, and also prevent them from upgrading to a newer version of OCP that would be incompatible with the version of your operator that is currently installed on their cluster.","title":"Overview"},{"location":"packaging-required-criteria-ocp/#configure-the-max-openshift-version-compatible","text":"Use the olm.openShiftMaxVersion annotation in the CSV to prevent the user from upgrading their OCP cluster before upgrading the installed operator version to any distribution which is compatible with: apiVersion : operators.coreos.com/v1alpha1 kind : ClusterServiceVersion metadata : annotations : # Prevent cluster upgrades to OpenShift Version 4.9 when this # bundle is installed on the cluster \"olm.properties\" : '[{\"type\": \"olm.maxOpenShiftVersion\", \"value\": \"4.8\"}]' The CSV annotation will eventually prevent the user from upgrading their OCP cluster before they have installed a version of your operator which is compatible with 4.9 . However, note that it is important to make these changes now as users running workloads with deprecated API(s) that are looking to upgrade to OCP 4.9 will need to be running operators that have this annotation set in order to prevent the cluster upgrade and potentially adversely impacting their crucial workloads. This option is useful when you know that the current version of your project will not work well on some specific Openshift version.","title":"Configure the max OpenShift Version compatible"},{"location":"packaging-required-criteria-ocp/#configure-the-openshift-distribution","text":"Use the annotation com.redhat.openshift.versions in bundle/metadata/annotations.yaml to ensure that the index image will be generated with its OCP Label, to prevent the bundle from being distributed on to 4.9: com.redhat.openshift.versions: \"v4.6-v4.8\" This option is also useful when you know that the current version of your project will not work well on some specific OpenShift version. By using it you defined the Openshift versions where the Operator should be distributed and the Operator will not appear in a catalog of an Openshift version that is outside of the range. You must use it if you are distributing a solution that contains deprecated API(s) and will no longer be available in later versions. For more information see Managing OpenShift Versions .","title":"Configure the Openshift distribution"},{"location":"packaging-required-criteria-ocp/#validate-the-bundle-with-the-common-criteria-to-distribute-via-olm-with-sdk","text":"Also, you can check the bundle via operator-sdk bundle validate against the suite Validator Community Operators and the K8s Version that you are intended to publish: operator-sdk bundle validate ./bundle --select-optional suite = operatorframework --optional-values = k8s-version = 1 .22 NOTE: The validators only check the manifests which are shipped in the bundle. They are unable to ensure that the project's code does not use the Deprecated/Removed API(s) in 1.22 and/or that it does not have as dependency another operator that uses them.","title":"Validate the bundle with the common criteria to distribute via OLM with SDK"},{"location":"packaging-required-criteria-ocp/#validate-the-bundle-with-the-specific-criteria-to-distribute-in-openshift-catalogs","text":"Pre-requirement Download the binary . You might want to keep it in your $GOPTH/bin Then, we can use the experimental OpenShift OLM Catalog Validator to check your Operator bundle. In this case, we need to inform the bundle and the annotations.yaml file paths: $ ocp-olm-catalog-validator my-bundle-path/bundle --optional-values=\"file=bundle-path/bundle/metadata/annotations.yaml\" Following is an example of an Operator bundle that uses the removed APIs in 1.22 and is not configured accordingly: $ ocp-olm-catalog-validator bundle/ --optional-values = \"file=bundle/metadata/annotations.yaml\" WARN [ 0000 ] Warning: Value memcached-operator.v0.0.1: this bundle is using APIs which were deprecated and removed in v1.22. More info: https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-22. Migrate the API ( s ) for CRD: ([ \"memcacheds.cache.example.com\" ]) ERRO [ 0000 ] Error: Value : ( memcached-operator.v0.0.1 ) olm.maxOpenShiftVersion csv.Annotations not specified with an OCP version lower than 4 .9. This annotation is required to prevent the user from upgrading their OCP cluster before they have installed a version of their operator which is compatible with 4 .9. For further information see https://docs.openshift.com/container-platform/4.8/operators/operator_sdk/osdk-working-bundle-images.html#osdk-control-compat_osdk-working-bundle-images ERRO [ 0000 ] Error: Value : ( memcached-operator.v0.0.1 ) this bundle is using APIs which were deprecated and removed in v1.22. More info: https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-22. Migrate the APIs for this bundle is using APIs which were deprecated and removed in v1.22. More info: https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-22. Migrate the API ( s ) for CRD: ([ \"memcacheds.cache.example.com\" ]) or provide compatible version ( s ) via the labels. ( e.g. LABEL com.redhat.openshift.versions = '4.6-4.8' )","title":"Validate the bundle with the specific criteria to distribute in Openshift catalogs"},{"location":"packaging-required-fields/","text":"Required fields within your CSV Preparing your CSV for use with OLM Before you begin, we strongly advise that you follow Operator-Lifecycle-Manager's docs on building a CSV for the Operator Framework . These outline the functional purpose of the CSV and which fields are required for installing your Operator CSV through OLM. Note that if you used operator-sdk to develop your Operator you can also leverage its packaging tooling to create a bundle by just running the target make bundle . Required fields for OperatorHub An Operator's CSV must contain the following fields and annotations for it to be displayed properly within OperatorHub.io and OperatorHub in OCP. Below is a guideline explaining each field, and at the bottom of this document is a full example of such a CSV. metadata : annotations : capabilities : One of the following : Basic Install, Seamless Upgrades, Full Lifecycle, Deep Insights, Auto Pilot. For more information see https://www.operatorhub.io/images/capability-level-diagram.svg categories : A comma separated list of categories from the values below. If not set, this will be set to \"Other\" in the UI containerImage : The repository that hosts the operator image. The format should match ${REGISTRYHOST}/${USERNAME}/${NAME}:${TAG} createdAt : The date that the operator was created. The format should match yyyy-mm-ddThh:mm:ssZ support : The name of the individual, company, or service that maintains this operator repository : (Optional) a URL to a source code repository of the Operator, intended for community Operators to direct users where to file issues / bug alm-examples : A string of a JSON list of example CRs for the operator's CRDs description : |- A short description of the operator that will be displayed on the marketplace tile If this annotation is not present, the `spec.description` value will be shown instead In either case, only the first 135 characters will appear spec : displayName : A short, readable name for the operator description : A detailed description of the operator, preferably in markdown format icon : - base64data : A base 64 representation of an image or logo associated with your operator mediatype : One of the following : image/png, image/jpeg, image/gif, image/svg+xml version : The operator version in semver format maintainers : - name : The name of the individual, company, or service that maintains this operator email : Email to reach maintainer provider : name : The name of the individual, company, or service that provides this operator links : - name : Title of the link (ex : Blog, Source Code etc.) url : url/link keywords : - 'A list of words that relate to your operator' - 'These are used when searching for operators in the UI' Logo requirements The logo for your Operator is inlined into the CSV as a base64-encoded string. The height:width ratio should be 1:2. The maximum dimensions are 80px for width and 40px in height. Categories For the best user experience, choose from the categories . If none of these categories fit your operator, please open a separate PR against categories.json . Once merged, you can open a PR with your operator assigned to your new category. Example CSV Below is an example of what the descheduler CSV may look like if it contained the expected annotations: apiVersion : operators.coreos.com/v1alpha1 kind : ClusterServiceVersion metadata : annotations : capabilities : Seamless Upgrades categories : \"OpenShift Optional\" containerImage : registry.svc.ci.openshift.org/openshift/origin-v4.0:descheduler-operator createdAt : 2019-01-01T11:59:59Z description : An operator to run the OpenShift descheduler repository : https://github.com/openshift/descheduler-operator alm-examples : | [ { \"apiVersion\": \"descheduler.io/v1alpha1\", \"kind\": \"Descheduler\", \"metadata\": { \"name\": \"example-descheduler-1\" }, \"spec\": { \"schedule\": \"*/1 * * * ?\", \"strategies\": [ { \"name\": \"lownodeutilization\", \"params\": [ { \"name\": \"cputhreshold\", \"value\": \"10\" }, { \"name\": \"memorythreshold\", \"value\": \"20\" }, { \"name\": \"memorytargetthreshold\", \"value\": \"30\" } ] } ] } } ] ... ... ... spec : displayName : Descheduler description : |- # Descheduler for Kubernetes ## Introduction Scheduling in Kubernetes is the process of binding pending pods to nodes, and is performed by a component of Kubernetes called kube-scheduler. The scheduler's decisions, whether or where a pod can or can not be scheduled, are guided by its configurable policy which comprises of set of rules, called predicates and priorities. The scheduler's decisions are influenced by its view of a Kubernetes cluster at that point of time when a new pod appears first time for scheduling. As Kubernetes clusters are very dynamic and their state change over time, there may be desired to move already running pods to some other nodes for various reasons * Some nodes are under or over utilized. * The original scheduling decision does not hold true any more, as taints or labels are added to or removed from nodes, pod/node affinity requirements are not satisfied any more. * Some nodes failed and their pods moved to other nodes. New nodes are added to clusters. Consequently, there might be several pods scheduled on less desired nodes in a cluster. Descheduler, based on its policy, finds pods that can be moved and evicts them. Please note, in current implementation, descheduler does not schedule replacement of evicted pods but relies on the default scheduler for that. ## Note Any api could be changed any time without any notice. That said, your feedback is very important and appreciated to make this project more stable and useful. icon : - base64data : this+is+a+base64-string== mediatype : image/png version : 0.0.1 provider : name : Red Hat, Inc. maintainers : - email : support@redhat.com name : Red Hat links : - name : GitHub Repository url : https://github.com/openshift/descheduler-operator keywords : [ 'deschedule' , 'scale' , 'binpack' , 'efficiency' ] ... ... ...","title":"Required fields"},{"location":"packaging-required-fields/#required-fields-within-your-csv","text":"","title":"Required fields within your CSV"},{"location":"packaging-required-fields/#preparing-your-csv-for-use-with-olm","text":"Before you begin, we strongly advise that you follow Operator-Lifecycle-Manager's docs on building a CSV for the Operator Framework . These outline the functional purpose of the CSV and which fields are required for installing your Operator CSV through OLM. Note that if you used operator-sdk to develop your Operator you can also leverage its packaging tooling to create a bundle by just running the target make bundle .","title":"Preparing your CSV for use with OLM"},{"location":"packaging-required-fields/#required-fields-for-operatorhub","text":"An Operator's CSV must contain the following fields and annotations for it to be displayed properly within OperatorHub.io and OperatorHub in OCP. Below is a guideline explaining each field, and at the bottom of this document is a full example of such a CSV. metadata : annotations : capabilities : One of the following : Basic Install, Seamless Upgrades, Full Lifecycle, Deep Insights, Auto Pilot. For more information see https://www.operatorhub.io/images/capability-level-diagram.svg categories : A comma separated list of categories from the values below. If not set, this will be set to \"Other\" in the UI containerImage : The repository that hosts the operator image. The format should match ${REGISTRYHOST}/${USERNAME}/${NAME}:${TAG} createdAt : The date that the operator was created. The format should match yyyy-mm-ddThh:mm:ssZ support : The name of the individual, company, or service that maintains this operator repository : (Optional) a URL to a source code repository of the Operator, intended for community Operators to direct users where to file issues / bug alm-examples : A string of a JSON list of example CRs for the operator's CRDs description : |- A short description of the operator that will be displayed on the marketplace tile If this annotation is not present, the `spec.description` value will be shown instead In either case, only the first 135 characters will appear spec : displayName : A short, readable name for the operator description : A detailed description of the operator, preferably in markdown format icon : - base64data : A base 64 representation of an image or logo associated with your operator mediatype : One of the following : image/png, image/jpeg, image/gif, image/svg+xml version : The operator version in semver format maintainers : - name : The name of the individual, company, or service that maintains this operator email : Email to reach maintainer provider : name : The name of the individual, company, or service that provides this operator links : - name : Title of the link (ex : Blog, Source Code etc.) url : url/link keywords : - 'A list of words that relate to your operator' - 'These are used when searching for operators in the UI'","title":"Required fields for OperatorHub"},{"location":"packaging-required-fields/#logo-requirements","text":"The logo for your Operator is inlined into the CSV as a base64-encoded string. The height:width ratio should be 1:2. The maximum dimensions are 80px for width and 40px in height.","title":"Logo requirements"},{"location":"packaging-required-fields/#categories","text":"For the best user experience, choose from the categories . If none of these categories fit your operator, please open a separate PR against categories.json . Once merged, you can open a PR with your operator assigned to your new category.","title":"Categories"},{"location":"packaging-required-fields/#example-csv","text":"Below is an example of what the descheduler CSV may look like if it contained the expected annotations: apiVersion : operators.coreos.com/v1alpha1 kind : ClusterServiceVersion metadata : annotations : capabilities : Seamless Upgrades categories : \"OpenShift Optional\" containerImage : registry.svc.ci.openshift.org/openshift/origin-v4.0:descheduler-operator createdAt : 2019-01-01T11:59:59Z description : An operator to run the OpenShift descheduler repository : https://github.com/openshift/descheduler-operator alm-examples : | [ { \"apiVersion\": \"descheduler.io/v1alpha1\", \"kind\": \"Descheduler\", \"metadata\": { \"name\": \"example-descheduler-1\" }, \"spec\": { \"schedule\": \"*/1 * * * ?\", \"strategies\": [ { \"name\": \"lownodeutilization\", \"params\": [ { \"name\": \"cputhreshold\", \"value\": \"10\" }, { \"name\": \"memorythreshold\", \"value\": \"20\" }, { \"name\": \"memorytargetthreshold\", \"value\": \"30\" } ] } ] } } ] ... ... ... spec : displayName : Descheduler description : |- # Descheduler for Kubernetes ## Introduction Scheduling in Kubernetes is the process of binding pending pods to nodes, and is performed by a component of Kubernetes called kube-scheduler. The scheduler's decisions, whether or where a pod can or can not be scheduled, are guided by its configurable policy which comprises of set of rules, called predicates and priorities. The scheduler's decisions are influenced by its view of a Kubernetes cluster at that point of time when a new pod appears first time for scheduling. As Kubernetes clusters are very dynamic and their state change over time, there may be desired to move already running pods to some other nodes for various reasons * Some nodes are under or over utilized. * The original scheduling decision does not hold true any more, as taints or labels are added to or removed from nodes, pod/node affinity requirements are not satisfied any more. * Some nodes failed and their pods moved to other nodes. New nodes are added to clusters. Consequently, there might be several pods scheduled on less desired nodes in a cluster. Descheduler, based on its policy, finds pods that can be moved and evicts them. Please note, in current implementation, descheduler does not schedule replacement of evicted pods but relies on the default scheduler for that. ## Note Any api could be changed any time without any notice. That said, your feedback is very important and appreciated to make this project more stable and useful. icon : - base64data : this+is+a+base64-string== mediatype : image/png version : 0.0.1 provider : name : Red Hat, Inc. maintainers : - email : support@redhat.com name : Red Hat links : - name : GitHub Repository url : https://github.com/openshift/descheduler-operator keywords : [ 'deschedule' , 'scale' , 'binpack' , 'efficiency' ] ... ... ...","title":"Example CSV"},{"location":"pull_request_template/","text":"Thanks submitting your Operator. Please check below list before you create your Pull Request. New Submissions [ ] Are you familiar with our contribution guidelines ? [ ] Have you packaged and deployed your Operator for Operator Framework? [ ] Have you tested your Operator with all Custom Resource Definitions? [ ] Have you tested your Operator in all supported installation modes ? [ ] Have you considered whether you want to use semantic versioning order ? [ ] Is your submission signed ? [ ] Is the operator icon set? Updates to existing Operators [ ] Did you create a ci.yaml file according to the update instructions ? [ ] Is your new CSV pointing to the previous version with the replaces property if you chose replaces-mode via the updateGraph property in ci.yaml ? [ ] Is your new CSV referenced in the appropriate channel defined in the package.yaml or annotations.yaml ? [ ] Have you tested an update to your Operator when deployed via OLM? [ ] Is your submission signed ? Your submission should not [ ] Modify more than one operator [ ] Modify an Operator you don't own [ ] Rename an operator - please remove and add with a different name instead [ ] Submit operators to both upstream-community-operators and community-operators at once [ ] Modify any files outside the above mentioned folders [ ] Contain more than one commit. Please squash your commits. Operator Description must contain (in order) [ ] Description about the managed Application and where to find more information [ ] Features and capabilities of your Operator and how to use it [ ] Any manual steps about potential pre-requisites for using your Operator Operator Metadata should contain [ ] Human readable name and 1-liner description about your Operator [ ] Valid category name 1 [ ] Links to the maintainer, source code and documentation [ ] Example templates for all Custom Resource Definitions intended to be used [ ] A quadratic logo Remember that you can preview your CSV here . -- 1 If you feel your Operator does not fit any of the pre-defined categories, file an issue against this repo and explain your need 2 For more information see here","title":"Pull request template"},{"location":"pull_request_template/#new-submissions","text":"[ ] Are you familiar with our contribution guidelines ? [ ] Have you packaged and deployed your Operator for Operator Framework? [ ] Have you tested your Operator with all Custom Resource Definitions? [ ] Have you tested your Operator in all supported installation modes ? [ ] Have you considered whether you want to use semantic versioning order ? [ ] Is your submission signed ? [ ] Is the operator icon set?","title":"New Submissions"},{"location":"pull_request_template/#updates-to-existing-operators","text":"[ ] Did you create a ci.yaml file according to the update instructions ? [ ] Is your new CSV pointing to the previous version with the replaces property if you chose replaces-mode via the updateGraph property in ci.yaml ? [ ] Is your new CSV referenced in the appropriate channel defined in the package.yaml or annotations.yaml ? [ ] Have you tested an update to your Operator when deployed via OLM? [ ] Is your submission signed ?","title":"Updates to existing Operators"},{"location":"pull_request_template/#your-submission-should-not","text":"[ ] Modify more than one operator [ ] Modify an Operator you don't own [ ] Rename an operator - please remove and add with a different name instead [ ] Submit operators to both upstream-community-operators and community-operators at once [ ] Modify any files outside the above mentioned folders [ ] Contain more than one commit. Please squash your commits.","title":"Your submission should not"},{"location":"pull_request_template/#operator-description-must-contain-in-order","text":"[ ] Description about the managed Application and where to find more information [ ] Features and capabilities of your Operator and how to use it [ ] Any manual steps about potential pre-requisites for using your Operator","title":"Operator Description must contain (in order)"},{"location":"pull_request_template/#operator-metadata-should-contain","text":"[ ] Human readable name and 1-liner description about your Operator [ ] Valid category name 1 [ ] Links to the maintainer, source code and documentation [ ] Example templates for all Custom Resource Definitions intended to be used [ ] A quadratic logo Remember that you can preview your CSV here . -- 1 If you feel your Operator does not fit any of the pre-defined categories, file an issue against this repo and explain your need 2 For more information see here","title":"Operator Metadata should contain"},{"location":"self-merge-updates/","text":"Publish Operator updates self-sufficiently Updating a published Operator is done by merging PR in to the main branch community-operators . By default, only community-operators maintainers can merge PRs to the main branch. They will do so if all validation and deployment tests done as part of the automatic checks running on every PR are successful. If you want to speed up the process of publishing an update, it is possible to have your PRs automatically merged without reviews by the maintainers. The following criteria need to be met: All GitHub checks are successful and package-validated label is set. Operator was successfully installed on Kubernetes or Openshift and installation-validated label is set. You are part of the reviewer group for the Operator in question ( more info ) Then authorized-changes label is set. If you are updating an already published Operator, only minor (cosmetic) changes are done ( more info ) No do-not-merge/hold , nor do-not-merge/work-in-progress label is set. The issue cannot be in draft mode If those criteria are fulfilled the PR will be automatically merged. Preventing automatic merging You can have a reason to prevent automatic merge. Just post /hold command/comment. Once your changes are final, post /hold cancel command/comment. Tests will be restarted and if all conditions stated above are met, merging automatically. How do I set up reviewers? Learn more here . Remember that modifications to ci.yaml need to be reviewed by current reviewers or the maintainers (if no reviewers exist). Every time ci.yaml is checked for reviewers, we are checking from operator-framework/community-operators default branch. So reviewers should be added in a separate PR in advance. How can I approve a PR against my operator? Auhtor is in reviewer list In this case, authorized_label is set automatically and PR will be auto merged when all tests will pass Author is not in reviewer list In this case, the reviewer can approve PR by GitHub PR review mechanizm . Since a reviewer doesn't have a write access to the repository it is not possible to set the label. But there are some ways to set authorized_changes label by our pipeline. The reviewer can do the following approve /hold /unhold or approve author will push some new changes or approve make draft Issue Ready for review Pipeline will set authorized_changes label only if last GitHub reviewer is in reviewer list (defined in ci.yaml ) and review state is APPROVED","title":"How do i deploy operator faster?"},{"location":"self-merge-updates/#publish-operator-updates-self-sufficiently","text":"Updating a published Operator is done by merging PR in to the main branch community-operators . By default, only community-operators maintainers can merge PRs to the main branch. They will do so if all validation and deployment tests done as part of the automatic checks running on every PR are successful. If you want to speed up the process of publishing an update, it is possible to have your PRs automatically merged without reviews by the maintainers. The following criteria need to be met: All GitHub checks are successful and package-validated label is set. Operator was successfully installed on Kubernetes or Openshift and installation-validated label is set. You are part of the reviewer group for the Operator in question ( more info ) Then authorized-changes label is set. If you are updating an already published Operator, only minor (cosmetic) changes are done ( more info ) No do-not-merge/hold , nor do-not-merge/work-in-progress label is set. The issue cannot be in draft mode If those criteria are fulfilled the PR will be automatically merged.","title":"Publish Operator updates self-sufficiently"},{"location":"self-merge-updates/#preventing-automatic-merging","text":"You can have a reason to prevent automatic merge. Just post /hold command/comment. Once your changes are final, post /hold cancel command/comment. Tests will be restarted and if all conditions stated above are met, merging automatically.","title":"Preventing automatic merging"},{"location":"self-merge-updates/#how-do-i-set-up-reviewers","text":"Learn more here . Remember that modifications to ci.yaml need to be reviewed by current reviewers or the maintainers (if no reviewers exist). Every time ci.yaml is checked for reviewers, we are checking from operator-framework/community-operators default branch. So reviewers should be added in a separate PR in advance.","title":"How do I set up reviewers?"},{"location":"self-merge-updates/#how-can-i-approve-a-pr-against-my-operator","text":"","title":"How can I approve a PR against my operator?"},{"location":"self-merge-updates/#auhtor-is-in-reviewer-list","text":"In this case, authorized_label is set automatically and PR will be auto merged when all tests will pass","title":"Auhtor is in reviewer list"},{"location":"self-merge-updates/#author-is-not-in-reviewer-list","text":"In this case, the reviewer can approve PR by GitHub PR review mechanizm . Since a reviewer doesn't have a write access to the repository it is not possible to set the label. But there are some ways to set authorized_changes label by our pipeline. The reviewer can do the following approve /hold /unhold or approve author will push some new changes or approve make draft Issue Ready for review Pipeline will set authorized_changes label only if last GitHub reviewer is in reviewer list (defined in ci.yaml ) and review state is APPROVED","title":"Author is not in reviewer list"},{"location":"stats/","text":"Community operators statistics History Number of PR over time Download : png pdf Download : png pdf Average PR merging time Download : png pdf Download : png pdf","title":"Statistics"},{"location":"stats/#community-operators-statistics","text":"","title":"Community operators statistics"},{"location":"stats/#history","text":"","title":"History"},{"location":"stats/#number-of-pr-over-time","text":"Download : png pdf Download : png pdf","title":"Number of PR over time"},{"location":"stats/#average-pr-merging-time","text":"Download : png pdf Download : png pdf","title":"Average PR merging time"},{"location":"testing-operators/","text":"Testing your Operator with Operator Framework Overview Accepted Contribution formats packagemanifest format (mandatory for all OLM versions prior to 0.14.0 and earlier, supported on all available versions) bundle format (supported with 0.14.0 or newer) Pre-Requisites Kubernetes cluster Container Tooling Operator Framework components Preparing file structure Operator Metadata Validation Temporary conversion of packagemanifest to bundle Using operator-sdk to validate your Operator Publishing your Operator metadata to a catalog for testing Building a catalog using packagemanifest format Building a catalog using bundles Testing Operator Deployment on Kubernetes 1. Installing Operator Lifecycle Manager Troubleshooting 2. Adding the catalog containing your Operator Troubleshooting 3. View Available Operators Troubleshooting 4. Create an OperatorGroup 5. Create a Subscription Troubleshooting 6. Verify Operator health Troubleshooting Testing Operator Deployment on OpenShift 1. Create the CatalogSource 2. Find your Operator in the OperatorHub UI 3. Install your Operator from OperatorHub 4. Verify Operator health Testing with scorecard Additional Resources Overview These instructions walk you through how to manually test that your Operator deploys correctly with Operator Framework when packaged for the Operator Lifecycle Manager. Although your submission will always be tested as part of the CI you can accelerate the process by testing locally. The tests described in this document can also be executed automatically in a single step using a test suite However, note that you can easily check and test your bundles via operator-sdk bundle validate : operator-sdk bundle validate ./bundle --select-optional suite = operatorframework NOTE For further information about the options available with operator-sdk bundle validate run operator-sdk bundle validate --list-optional and operator-sdk bundle validate --help . And then, if you used operator-sdk to build your project and bundle you can leverage in operator-sdk scorecard : operator-sdk scorecard bundle FOR K8S COMMUNITY OPERATORS (OperatorHub.io) For K8s community operators make sure you understand and performed the required additional checks before publishing your PR in the repository k8s-operatorhub/community-operators . to distribute your Operator in OperatorHub.io . In this case, you can verify it specific criteria to distribute your Operator in this Index Catalog by also checking your bundle with the K8S Community Bundle Validator , e.g: k8s-community-bundle-validator <bundle-path> FOR OPENSHIFT COMMUNITY OPERATORS For Openshift community operators make sure you understand and performed the required additional checks before publishing your PR in the repository redhat-openshift-ecosystem/community-operators-prod to distribute your Operator in Openshift and OKD catalogs. See OKD/OpenShift Catalogs criteria and options . Accepted Contribution formats The process below assumes that you have a Kubernetes Operator in either of the two following formats supported by the Operator Framework: bundle format (supported with 0.14.0 or newer) $ tree my-operator/ my-operator \u251c\u2500\u2500 0.1.0 \u2502 \u251c\u2500\u2500 manifests \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2502 \u2514\u2500\u2500 my-operator.v0.1.0.clusterserviceversion.yaml \u2502 \u251c\u2500\u2500 metadata \u2502 \u2502 \u2514\u2500\u2500 annotations.yaml \u2502 \u2514\u2500\u2500 Dockerfile \u251c\u2500\u2500 0.5.0 \u2502 \u251c\u2500\u2500 manifests \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2502 \u2514\u2500\u2500 my-operator.v0.5.0.clusterserviceversion.yaml \u2502 \u251c\u2500\u2500 metadata \u2502 \u2502 \u2514\u2500\u2500 annotations.yaml \u2502 \u2514\u2500\u2500 Dockerfile \u2514\u2500\u2500 1.0.0 \u251c\u2500\u2500 manifests \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v1.0.0.clusterserviceversion.yaml \u251c\u2500\u2500 metadata \u2502 \u2514\u2500\u2500 annotations.yaml \u2514\u2500\u2500 Dockerfile ... (Legacy) packagemanifest format (mandatory for all OLM versions prior to 0.14.0 and earlier, supported on all available versions) NOTE It is recommended to use the bundle format instead. This format is still valid for backward compatibility only and at some point will no longer be supported. $ tree my-operator/ my-operator \u251c\u2500\u2500 0.1.0 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v0.1.0.clusterserviceversion.yaml \u251c\u2500\u2500 0.5.0 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v0.5.0.clusterserviceversion.yaml \u251c\u2500\u2500 1.0.0 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v1.0.0.clusterserviceversion.yaml \u2514\u2500\u2500 my-operator.package.yaml In both examples above my-operator is the name of your Operator which is available in 3 versions: 0.1.0 , 0.5.0 and 1.0.0 . If you are new to this or don't have this format yet, refer to our contribution documentation . We will refer to both formats distinctively below where required. Mixing packagemanifest style and bundle format style Operator versions in a single Operator package is not supported . All versions all need to be in either one or the other format. Pre-Requisites Kubernetes cluster For \" upstream-community \" operators targeting Kubernetes and OperatorHub.io : * A running Kubernetes cluster; minikube or Kubernetes-in-Docker is the simplest approach For \" community \" operators targeting OCP/OKD and OperatorHub on OpenShift: * access to a running production-like OpenShift 4 cluster, use try.openshift.com to get a cluster on an infrastructure of your choice * or access to an all-in-one OpenShift 4 cluster, use CodeReady Containers to get a cluster on your local machine Container Tooling You need an OCI-compatible container toolchain on the system where you test your Operator. Support options are: podman buildah moby (aka docker-ce aka docker ) Operator Framework components The following parts of the framework are used throughout the process and should be downloaded and put in your executable search path (Linux and Mac are supported): opm operator-sdk Preparing file structure Finally, if you haven't already done so, please clone this repository as well and create a branch: git clone https://github.com/operator-framework/community-operators.git cd community-operators/ git branch my-operator For simplicity (and if your Operator has dependencies on other community Operators) put your my-operator directory in either of the community-operators (for OpenShift's OperatorHub) or upstream-community-operators (for OperatorHub.io) directory (or both). cp -R my-operator/ community-operators/upstream-community-operators/ The name of the directory my-operator/ needs to match the Operator package name (without the slash) in either package.yaml (if you are using the packagemanifest format) or the container image label operators.operatorframework.io.bundle.package.v1 in the Dockerfile and annotations.yaml (if you are using the bundle format). If you are just adding a new version of your Operator, please create a subdirectory following semver conventions in your existing package directory, for example: cp -R my-operator/2.0.0 community-operators/upstream-community-operators/my-operator/ If you are using the packagemanifest format, don't forget to update the package.yaml file in the top-level directory to point to your new version/channels. Operator Metadata Validation If you are using packagemanifest format you will need to convert your metadata to bundle format for the validation step. Temporary conversion of packagemanifest to bundle Suppose v2.0.0 is the version of the Operator you want to test convert to bundle format directory with the opm tool: mkdir /tmp/my-operator-2.0.0-bundle/ cd /tmp/my-operator-2.0.0-bundle/ opm alpha bundle build --directory /path/to/my-operator/2.0.0/ --tag my-operator-bundle:v2.0.0 --output-dir . This will have generated the bundle format layout in the current working directory /tmp/my-operator-2.0.0-bundle/ : $ tree . /tmp/my-operator-2.0.0-bundle/ \u251c\u2500\u2500 manifests \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v2.0.0.clusterserviceversion.yaml \u251c\u2500\u2500 metadata \u2502 \u2514\u2500\u2500 annotations.yaml \u2514\u2500\u2500 bundle.Dockerfile Run the following validation command of the operator-sdk from within this directory. operator-sdk bundle validate /tmp/my-operator-2.0.0-bundle/ --select-optional name=operatorhub Using operator-sdk to validate your Operator Validation using operator-sdk is only supported using the bundle format layout. See the previous step if you need to convert from packagemanifest . Validation is done on a per-Operator version basis. If you are not already in the Operator versions directory, switch to it now, e.g. cd my-operator/2.0.0/ With the Operator in bundle format use the operator-sdk to validate your bundle with the additional rules for community submissions: operator-sdk bundle validate --select-optional name=operatorhub . The output might look similar to this: INFO[0000] Found annotations file bundle-dir=. container-tool=docker INFO[0000] Could not find optional dependencies file bundle-dir=. container-tool=docker INFO[0000] All validation tests have completed successfully If there are any errors or warnings they will be displayed there. The container-tool will be automatically determined given your environment. If you want to force to use podman instead of docker , supply the -b switch to the operator-sdk bundle validate command. Any warnings here might turn into failing pipeline tests here. Please correct all issues displayed. A list of fields that are scanned can also be reviewed with this list . Publishing your Operator metadata to a catalog for testing Building a catalog using packagemanifest format When your Operator metadata is formatted in packagemanifest layout you need to place it in the directory structure of the community-operators repository, according to pre-requisites step . For example, assuming version 2.0.0 is the version you like to test: $ tree upstream-community-operators/ upstream-community-operators/ \u2502 ... \u2502 \u2514\u2500\u2500my-operator/ \u251c\u2500\u2500 0.1.0 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v0.1.0.clusterserviceversion.yaml \u251c\u2500\u2500 0.5.0 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v0.5.0.clusterserviceversion.yaml \u251c\u2500\u2500 1.0.0 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v1.0.0.clusterserviceversion.yaml \u251c\u2500\u2500 2.0.0 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v2.0.0.clusterserviceversion.yaml \u2514\u2500\u2500 my-operator.package.yaml You can build a catalog for OLM containing either all Operators or just yours with a Dockerfile like this FROM quay.io/operator-framework/upstream-registry-builder as builder COPY upstream-community-operators manifests RUN /bin/initializer -o ./bundles.db FROM scratch COPY --from = builder /etc/nsswitch.conf /etc/nsswitch.conf COPY --from = builder /bundles.db /bundles.db COPY --from = builder /bin/registry-server /registry-server COPY --from = builder /bin/grpc_health_probe /bin/grpc_health_probe EXPOSE 50051 ENTRYPOINT [ \"/registry-server\" ] CMD [ \"--database\" , \"bundles.db\" ] Simply adjust the second line to either include all OpenShift Community Operators ( community-operators ), all OperatorHub.io Operators ( upstream-community-operators ) or just your Operator (e.g. upstream-community-operator/my-operator ). Place the Dockerfile in the top-level directory of your cloned copy of this repo, build it and push it to a registry from where you can download it to your Kubernetes cluster later. For example: podman build -f catalog.Dockerfile -t my-test-catalog:latest . podman tag my-test-catalog:latest quay.io/myaccount/my-test-catalog:latest podman push quay.io/myaccount/my-test-catalog:latest Building a catalog using bundles When your Operator metadata is formatted in bundle layout you can optionally add it to the existing directory structure like described in the pre-requisites step . For building a catalog this is not required because with Operator bundles versions are incrementally added to an existing or empty catalog. For example, assuming version 2.0.0 is the version you like to test: $ tree my-operator/ my-operator/ \u251c\u2500\u2500 0.1.0 \u2502 \u251c\u2500\u2500 manifests \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2502 \u2514\u2500\u2500 my-operator.v0.1.0.clusterserviceversion.yaml \u2502 \u251c\u2500\u2500 metadata \u2502 \u2502 \u2514\u2500\u2500 annotations.yaml \u2502 \u2514\u2500\u2500 Dockerfile \u251c\u2500\u2500 0.5.0 \u2502 \u251c\u2500\u2500 manifests \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2502 \u2514\u2500\u2500 my-operator.v0.5.0.clusterserviceversion.yaml \u2502 \u251c\u2500\u2500 metadata \u2502 \u2502 \u2514\u2500\u2500 annotations.yaml \u2502 \u2514\u2500\u2500 Dockerfile \u251c\u2500\u2500 1.0.0 \u2502 \u251c\u2500\u2500 manifests \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2502 \u2514\u2500\u2500 my-operator.v1.0.0.clusterserviceversion.yaml \u2502 \u251c\u2500\u2500 metadata \u2502 \u2502 \u2514\u2500\u2500 annotations.yaml \u2502 \u2514\u2500\u2500 Dockerfile \u2514\u2500\u2500 2.0.0 \u251c\u2500\u2500 manifests \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v2.0.0.clusterserviceversion.yaml \u251c\u2500\u2500 metadata \u2502 \u2514\u2500\u2500 annotations.yaml \u2514\u2500\u2500 Dockerfile ... Simply build your bundle using the Dockerfile that is part of every Operator bundle. If you are new to this format please consult the operator-registry documentation. If you used operator-sdk to develop your Operator you can also leverage its packaging tooling to create a bundle . To build your bundle simply build the image and push it to a registry of your choice: podman build -f 2.0.0/Dockerfile -t my-operator:v2.0.0 2.0.0/ podman push my-operator:v2.0.0 quay.io/myaccount/my-operator:v2.0.0 With the bundle published to a registry you can now leverage opm to add it to the existing catalog of community operators: for OpenShift this is quay.io/openshift-community-operators/catalog:latest for OperatorHub.io this is quay.io/operatorhubio/catalog:latest opm will create a catalog image with your Operator added, like so: opm index add --bundles quay.io/myaccount/my-operator:v2.0.0 --from-index quay.io/operatorhubio/catalog:latest --tag quay.io/myaccount/my-test-catalog:latest You then push the resulting catalog image to a registry of your choice as well: podman push quay.io/myaccount/my-test-catalog:latest opm also supports multiple container tools via the -c switch. You can also omit the final step to build the catalog image and instead output the Dockerfile that would be used. Consult the help output for that via opm index add --help You now have a catalog image available for OLM to install your Operator version from. Testing Operator Deployment on Kubernetes 1. Installing Operator Lifecycle Manager If you are not using OpenShift you first need to install the Operator Lifecycle Manager on your cluster. The following steps assume you already have a running Kubernetes cluster that is currently selected as your current-context via kubectl . If you, you can quickly spin up a cluster using tools like KIND or minikube mentioned in the pre-requisites section , e.g. kind create cluster You can install the Operator Lifecycle Manager using either operator-sdk or kubectl . Option 1: Install the Operator Lifecycle Manager using operator-sdk : operator-sdk olm install Verify that OLM is installed correctly: operator-sdk olm status This should output something like the following: INFO[0000] Fetching CRDs for version \"0.16.1\" INFO[0000] Fetching resources for version \"0.16.1\" INFO[0002] Successfully got OLM status for version \"0.16.1\" NAME NAMESPACE KIND STATUS operators.operators.coreos.com CustomResourceDefinition Installed operatorgroups.operators.coreos.com CustomResourceDefinition Installed installplans.operators.coreos.com CustomResourceDefinition Installed clusterserviceversions.operators.coreos.com CustomResourceDefinition Installed olm-operator olm Deployment Installed subscriptions.operators.coreos.com CustomResourceDefinition Installed olm-operator-binding-olm ClusterRoleBinding Installed operatorhubio-catalog olm CatalogSource Installed olm-operators olm OperatorGroup Installed aggregate-olm-view ClusterRole Installed catalog-operator olm Deployment Installed aggregate-olm-edit ClusterRole Installed olm Namespace Installed global-operators operators OperatorGroup Installed operators Namespace Installed packageserver olm ClusterServiceVersion Installed olm-operator-serviceaccount olm ServiceAccount Installed catalogsources.operators.coreos.com CustomResourceDefinition Installed system:controller:operator-lifecycle-manager ClusterRole Installed Option 2: Install the Operator Lifecycle Manager using kubectl : kubectl apply -f https://github.com/operator-framework/operator-lifecycle-manager/releases/download/v0.17.0/crds.yaml kubectl apply -f https://github.com/operator-framework/operator-lifecycle-manager/releases/download/v0.17.0/olm.yaml You can check if Operator Lifecycle Manager status via the following command: kubectl get pods -n olm This should output something like the following: NAME READY STATUS RESTARTS AGE catalog-operator-7b4788ffb5-9jggk 1/1 Running 0 23h olm-operator-678d76b95c-r492j 1/1 Running 0 23h operatorhubio-catalog-lqw72 1/1 Running 0 23h packageserver-7cfd786c67-pwpnn 1/1 Running 0 23h packageserver-7cfd786c67-tkwbm 1/1 Running 0 23h Troubleshooting If any problems are encountered at this step, verify that you have enough permissions to install OLM (you need to be cluster-admin to register its CRDs) and create an issue in the OLM tracker . 2. Adding the catalog containing your Operator Create a CatalogSource instance in the olm namespace to reference in the Operator catalog image that contains your Operator version to test: apiVersion : operators.coreos.com/v1alpha1 kind : CatalogSource metadata : name : my-test-catalog namespace : olm spec : sourceType : grpc image : quay.io/myaccount/my-test-catalog:latest Deploy the CatalogSource resource: kubectl apply -f catalog-source.yaml If you created your test catalog containing all existing community operators , you should delete the default catalog that OLM ships with to avoid a lot of duplicate entries: kubectl delete catalogsource operatorhubio-catalog -n olm Verify your custom catalog got loaded: $ kubectl get catalogsource -n olm NAME DISPLAY TYPE PUBLISHER AGE my-test-catalog grpc 3m32s [...] Verify the health of your catalog: kubectl get catalogsource my-test-catalog -n olm -o yaml Troubleshooting The status section of that object has the lastObservedState condition set to READY . If that is not the case (for example if the condition is set to CONNECTING ) check the healthiness of the pod associated with the catalog in the same namespace. kubectl get pod -n olm The name of the pod will carry the name of the CatalogSource object plus 5 random characters. Usually, the source of unhealthy catalogs is catalog pods with pull errors due to missing authentication against the registry or non-existent tags. If the pod is currently running check its logs: kubectl logs my-test-catalog-zcq7h -n olm If there are errors in this log please raise them in the operator-registry issue tracker as any problems caused by malformed bundle/packagemanifest metadata should have been caught during catalog creation. 3. View Available Operators Inspect the list of loaded packagemanifests on the system with the following command to filter for your Operator kubectl get packagemanifests | grep my-operator The example should look like this: grep my-operator 1h2m If your Operator appears in this list, the catalog was successfully parsed and the Operator is now available to install. Troubleshooting If it does not appear in this list return to the previous step and check the logs of the catalog pod. If this does not reveal any error messages check the log of both the packageserver pods of OLM in the olm namespace, e.g.: kubectl get logs packageserver-78c99949df-lf26p -n olm On some occasions the Operator definition is in the catalog but cannot be understood due to some malformed package/bundle content. In this case, the packageserver should present a related error message. If there are errors in this log please raise them in the operator-registry issue tracker as any problems caused by malformed bundle/packagemanifest metadata should have been caught during catalog creation. 4. Create an OperatorGroup An OperatorGroup is used to denote which namespaces your Operator should be watching. It must exist in the namespace where your operator should be deployed, we'll use default in this example. Its configuration depends on whether your Operator supports watching its own namespace, a single other namespace or all namespaces (as indicated by spec.installModes in the CSV). Create the following file as operator-group.yaml if your Operator supports watching its own or a single namespace. If your Operator supports watching all namespaces you can skip this step and proceed to creating the Subscription object in the operators namespace. An OperatorGroup already exists there with spec.targetNamespace empty. This kind of OperatorGroup instructs the Operator to watch all namespaces. apiVersion : operators.coreos.com/v1alpha2 kind : OperatorGroup metadata : name : my-operatorgroup namespace : default spec : targetNamespaces : - default Deploy the OperatorGroup resource: kubectl apply -f operator-group.yaml 5. Create a Subscription The last step is to ask OLM to install your Operator. A Subscription is created to represent the intent to install an Operator and keep it updated (automatically even) with a newer version from the catalog. This requires you to tell OLM which Operator, in which version from which channel you want to install and where the catalog is, that contains the Operator. Save the following to a file named: operator-subscription.yaml : apiVersion : operators.coreos.com/v1alpha1 kind : Subscription metadata : name : my-operator-subscription namespace : default spec : channel : <channel-name> name : my-operator startingCSV : my-operator.v2.0.0 source : my-test-catalog sourceNamespace : olm If your Operator supports watching all namespaces, change the Subscription namespace from default to operators . (This namespace already has an OperatorGroup ). In any case, replace <channel-name> with the contents of channel.name in your package.yaml file if you have the packagemanifest format or with the contents from operators.operatorframework.io.bundle.channels.v1 in annotations.yaml if you have the bundle format. Then create the Subscription : kubectl apply -f operator-subscription.yaml Troubleshooting Note that the Subscription object is not representing your installed Operator (this is the object in the next step). It only instructs OLM to install your Operator and keep it updated (automatically). Any error message in the Subscription status refers to the attempt to install the Operator from the catalog, which is usually caused by incorrect references to the catalog (types in the name of the Operator, catalog, namespace, etc). 6. Verify Operator health Watch your Operator being deployed by OLM from the catalog with the following command. Change the namespace from default to operators if you installed your Subscrption there. Use the watch switch -w : $ kubectl get clusterserviceversion -n default -w NAME DISPLAY VERSION REPLACES PHASE my-operator.v2.0.0 My Operator 2.0.0 my-operator.v1.0.0 my-operator.v2.0.0 My Operator 2.0.0 my-operator.v1.0.0 my-operator.v2.0.0 My Operator 2.0.0 my-operator.v1.0.0 my-operator.v2.0.0 My Operator 2.0.0 my-operator.v1.0.0 Pending my-operator.v2.0.0 My Operator 2.0.0 my-operator.v1.0.0 InstallReady my-operator.v2.0.0 My Operator 2.0.0 my-operator.v1.0.0 Installing my-operator.v2.0.0 My Operator 2.0.0 my-operator.v1.0.0 Installing my-operator.v2.0.0 My Operator 2.0.0 my-operator.v1.0.0 Installing my-operator.v2.0.0 My Operator 2.0.0 my-operator.v1.0.0 Installing my-operator.v2.0.0 My Operator 2.0.0 my-operator.v1.0.0 Succeeded The ClusterServiceVersion object represents your installed Operator per version. It can take a couple of seconds to be created as part of the Subscription request. There will always be a ClusterServiceVersion in the namespace of the Subscription . If the OperatorGroup was configured to \"watch\" a list of all namespaces, the ClusterServiceVersion object will be copied to all those namespaces. This might take additional time (usually around 30 secs). If the above command succeeds and the ClusterServiceVersion has transitioned to the Succeeded phase you will also find your Operator's Deployment(s) in the same namespace where the Subscription is. This is your Operator running: kubectl get deployment -n default Troubleshooting If the ClusterServiceVersion is in a pending or failed state, problems occurred when trying to install the Operator. There are two common sources: the components that make up the Operator and the Operator binary itself. Problems with the Operator process itself will result in a Deployment that is unhealthy, either due to a crashing Operator pod or other image level problems. In this case, debug the Deployment and Operator logs for any error message. Usually, there are either bugs in the Operator or insufficient permissions in the RBAC part of the bundle/package metadata which may crash poorly written Operators. Problems with installing the Operator components can be debugged with the InstallPlan object. It contains a blueprint of the Operator and lists all the Kubernetes resources that are required for the Operator to function. It is automatically created by OLM and linked to the Subscription . If the Subscription created an InstallPlan you can refer to it via the status block of the Subscription : .status.InstallPlan contains the name of the InstallPlan object which is always in the same namespace as the Subscription . kubectl describe subscription my-operator-subscription -n default ... Status: ... Installplan: API Version: operators.coreos.com/v1alpha1 Kind: InstallPlan Name: install-2c8lf Uuid: 656e2e6b-582a-46e1-867d-4f7e95c24136 Last Updated: 2020-11-01T19:38:01Z State: AtLatestKnown Events: <none> kubectl describe installplan install-2c8lf -n default This will likely be a lengthy output due to the content of every component of the Operator returned. However, in the .status block of this object, the name and health of every component is displayed. Problems with InstallPlan components usually stem from malformed components, insufficient permissions, collisions with existing objects etc. It usually needs to be corrected at the Operator metadata level. Testing Operator Deployment on OpenShift The process to test on OpenShift Container Platform and OKD 4.3 or newer is exactly the same as described above with the exception that OLM is already installed. You can use the same CLI steps to test your Operator but it can however also be done via the UI. 1. Create the CatalogSource Create a CatalogSource instance in the openshift-marketplace namespace to reference the Operator catalog image that contains your Operator version to test: apiVersion : operators.coreos.com/v1alpha1 kind : CatalogSource metadata : name : my-test-catalog namespace : openshift-marketplace spec : sourceType : grpc image : quay.io/myaccount/my-test-catalog:latest You can use the OpenShift Console YAML editor for that or deploy the CatalogSource resource on the CLI: oc apply -f catalog-source.yaml If you created your test catalog containing all existing community operators , you should delete the default catalog that OLM ships with to avoid a lot of duplicate entries. To do this navigate to Administration on the main navigation bar on the left and select the Cluster Settings item. In this view, select the Global Configuration tab and filter for OperatorHub : Click on the OperatorHub item and select the YAML tab. In the YAML editor set the disabled property for the community-catalog to true : Click Save . 2. Find your Operator in the OperatorHub UI Go to your OpenShift UI and find your Operator by filtering for the Custom category: 3. Install your Operator from OperatorHub To install your Operator simply click its icon and in the proceeding dialog click Install . You will be asked where to install your Operator. Select either of the desired installation modes, if your Operator supports it and then click Subscribe You will be forwarded to the Subscription Management section of the OLM UI and after a couple of moments, your Operator will be transitioning to Installed . 4. Verify Operator health Change to the Installed Operators section in the left-hand navigation menu to verify your Operator's installation status: It should have transitioned into the state InstallationSucceeded . You can now test it by starting to use its APIs. Testing with scorecard If your Operator is up and running you can verify it is working as intended using its APIs. Additionally, you can run operator-sdk 's scorecard utility for validating against good practice and correctness of your Operator. Assuming you are still in your top-level directory where my-operator/ is your bundle location follow these instructions to test your Operator using the Operator-SDK : Running your Operator with scorecard Additional Resources Cluster Service Version Spec Example Bundle","title":"Explained"},{"location":"testing-operators/#testing-your-operator-with-operator-framework","text":"Overview Accepted Contribution formats packagemanifest format (mandatory for all OLM versions prior to 0.14.0 and earlier, supported on all available versions) bundle format (supported with 0.14.0 or newer) Pre-Requisites Kubernetes cluster Container Tooling Operator Framework components Preparing file structure Operator Metadata Validation Temporary conversion of packagemanifest to bundle Using operator-sdk to validate your Operator Publishing your Operator metadata to a catalog for testing Building a catalog using packagemanifest format Building a catalog using bundles Testing Operator Deployment on Kubernetes 1. Installing Operator Lifecycle Manager Troubleshooting 2. Adding the catalog containing your Operator Troubleshooting 3. View Available Operators Troubleshooting 4. Create an OperatorGroup 5. Create a Subscription Troubleshooting 6. Verify Operator health Troubleshooting Testing Operator Deployment on OpenShift 1. Create the CatalogSource 2. Find your Operator in the OperatorHub UI 3. Install your Operator from OperatorHub 4. Verify Operator health Testing with scorecard Additional Resources","title":"Testing your Operator with Operator Framework"},{"location":"testing-operators/#overview","text":"These instructions walk you through how to manually test that your Operator deploys correctly with Operator Framework when packaged for the Operator Lifecycle Manager. Although your submission will always be tested as part of the CI you can accelerate the process by testing locally. The tests described in this document can also be executed automatically in a single step using a test suite However, note that you can easily check and test your bundles via operator-sdk bundle validate : operator-sdk bundle validate ./bundle --select-optional suite = operatorframework NOTE For further information about the options available with operator-sdk bundle validate run operator-sdk bundle validate --list-optional and operator-sdk bundle validate --help . And then, if you used operator-sdk to build your project and bundle you can leverage in operator-sdk scorecard : operator-sdk scorecard bundle","title":"Overview"},{"location":"testing-operators/#for-k8s-community-operators-operatorhubio","text":"For K8s community operators make sure you understand and performed the required additional checks before publishing your PR in the repository k8s-operatorhub/community-operators . to distribute your Operator in OperatorHub.io . In this case, you can verify it specific criteria to distribute your Operator in this Index Catalog by also checking your bundle with the K8S Community Bundle Validator , e.g: k8s-community-bundle-validator <bundle-path>","title":"FOR K8S COMMUNITY OPERATORS (OperatorHub.io)"},{"location":"testing-operators/#for-openshift-community-operators","text":"For Openshift community operators make sure you understand and performed the required additional checks before publishing your PR in the repository redhat-openshift-ecosystem/community-operators-prod to distribute your Operator in Openshift and OKD catalogs. See OKD/OpenShift Catalogs criteria and options .","title":"FOR OPENSHIFT COMMUNITY OPERATORS"},{"location":"testing-operators/#accepted-contribution-formats","text":"The process below assumes that you have a Kubernetes Operator in either of the two following formats supported by the Operator Framework:","title":"Accepted Contribution formats"},{"location":"testing-operators/#bundle-format-supported-with-0140-or-newer","text":"$ tree my-operator/ my-operator \u251c\u2500\u2500 0.1.0 \u2502 \u251c\u2500\u2500 manifests \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2502 \u2514\u2500\u2500 my-operator.v0.1.0.clusterserviceversion.yaml \u2502 \u251c\u2500\u2500 metadata \u2502 \u2502 \u2514\u2500\u2500 annotations.yaml \u2502 \u2514\u2500\u2500 Dockerfile \u251c\u2500\u2500 0.5.0 \u2502 \u251c\u2500\u2500 manifests \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2502 \u2514\u2500\u2500 my-operator.v0.5.0.clusterserviceversion.yaml \u2502 \u251c\u2500\u2500 metadata \u2502 \u2502 \u2514\u2500\u2500 annotations.yaml \u2502 \u2514\u2500\u2500 Dockerfile \u2514\u2500\u2500 1.0.0 \u251c\u2500\u2500 manifests \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v1.0.0.clusterserviceversion.yaml \u251c\u2500\u2500 metadata \u2502 \u2514\u2500\u2500 annotations.yaml \u2514\u2500\u2500 Dockerfile ...","title":"bundle format (supported with 0.14.0 or newer)"},{"location":"testing-operators/#legacy-packagemanifest-format-mandatory-for-all-olm-versions-prior-to-0140-and-earlier-supported-on-all-available-versions","text":"NOTE It is recommended to use the bundle format instead. This format is still valid for backward compatibility only and at some point will no longer be supported. $ tree my-operator/ my-operator \u251c\u2500\u2500 0.1.0 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v0.1.0.clusterserviceversion.yaml \u251c\u2500\u2500 0.5.0 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v0.5.0.clusterserviceversion.yaml \u251c\u2500\u2500 1.0.0 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v1.0.0.clusterserviceversion.yaml \u2514\u2500\u2500 my-operator.package.yaml In both examples above my-operator is the name of your Operator which is available in 3 versions: 0.1.0 , 0.5.0 and 1.0.0 . If you are new to this or don't have this format yet, refer to our contribution documentation . We will refer to both formats distinctively below where required. Mixing packagemanifest style and bundle format style Operator versions in a single Operator package is not supported . All versions all need to be in either one or the other format.","title":"(Legacy) packagemanifest format (mandatory for all OLM versions prior to 0.14.0 and earlier, supported on all available versions)"},{"location":"testing-operators/#pre-requisites","text":"","title":"Pre-Requisites"},{"location":"testing-operators/#kubernetes-cluster","text":"For \" upstream-community \" operators targeting Kubernetes and OperatorHub.io : * A running Kubernetes cluster; minikube or Kubernetes-in-Docker is the simplest approach For \" community \" operators targeting OCP/OKD and OperatorHub on OpenShift: * access to a running production-like OpenShift 4 cluster, use try.openshift.com to get a cluster on an infrastructure of your choice * or access to an all-in-one OpenShift 4 cluster, use CodeReady Containers to get a cluster on your local machine","title":"Kubernetes cluster"},{"location":"testing-operators/#container-tooling","text":"You need an OCI-compatible container toolchain on the system where you test your Operator. Support options are: podman buildah moby (aka docker-ce aka docker )","title":"Container Tooling"},{"location":"testing-operators/#operator-framework-components","text":"The following parts of the framework are used throughout the process and should be downloaded and put in your executable search path (Linux and Mac are supported): opm operator-sdk","title":"Operator Framework components"},{"location":"testing-operators/#preparing-file-structure","text":"Finally, if you haven't already done so, please clone this repository as well and create a branch: git clone https://github.com/operator-framework/community-operators.git cd community-operators/ git branch my-operator For simplicity (and if your Operator has dependencies on other community Operators) put your my-operator directory in either of the community-operators (for OpenShift's OperatorHub) or upstream-community-operators (for OperatorHub.io) directory (or both). cp -R my-operator/ community-operators/upstream-community-operators/ The name of the directory my-operator/ needs to match the Operator package name (without the slash) in either package.yaml (if you are using the packagemanifest format) or the container image label operators.operatorframework.io.bundle.package.v1 in the Dockerfile and annotations.yaml (if you are using the bundle format). If you are just adding a new version of your Operator, please create a subdirectory following semver conventions in your existing package directory, for example: cp -R my-operator/2.0.0 community-operators/upstream-community-operators/my-operator/ If you are using the packagemanifest format, don't forget to update the package.yaml file in the top-level directory to point to your new version/channels.","title":"Preparing file structure"},{"location":"testing-operators/#operator-metadata-validation","text":"If you are using packagemanifest format you will need to convert your metadata to bundle format for the validation step.","title":"Operator Metadata Validation"},{"location":"testing-operators/#temporary-conversion-of-packagemanifest-to-bundle","text":"Suppose v2.0.0 is the version of the Operator you want to test convert to bundle format directory with the opm tool: mkdir /tmp/my-operator-2.0.0-bundle/ cd /tmp/my-operator-2.0.0-bundle/ opm alpha bundle build --directory /path/to/my-operator/2.0.0/ --tag my-operator-bundle:v2.0.0 --output-dir . This will have generated the bundle format layout in the current working directory /tmp/my-operator-2.0.0-bundle/ : $ tree . /tmp/my-operator-2.0.0-bundle/ \u251c\u2500\u2500 manifests \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v2.0.0.clusterserviceversion.yaml \u251c\u2500\u2500 metadata \u2502 \u2514\u2500\u2500 annotations.yaml \u2514\u2500\u2500 bundle.Dockerfile Run the following validation command of the operator-sdk from within this directory. operator-sdk bundle validate /tmp/my-operator-2.0.0-bundle/ --select-optional name=operatorhub","title":"Temporary conversion of packagemanifest to bundle"},{"location":"testing-operators/#using-operator-sdk-to-validate-your-operator","text":"Validation using operator-sdk is only supported using the bundle format layout. See the previous step if you need to convert from packagemanifest . Validation is done on a per-Operator version basis. If you are not already in the Operator versions directory, switch to it now, e.g. cd my-operator/2.0.0/ With the Operator in bundle format use the operator-sdk to validate your bundle with the additional rules for community submissions: operator-sdk bundle validate --select-optional name=operatorhub . The output might look similar to this: INFO[0000] Found annotations file bundle-dir=. container-tool=docker INFO[0000] Could not find optional dependencies file bundle-dir=. container-tool=docker INFO[0000] All validation tests have completed successfully If there are any errors or warnings they will be displayed there. The container-tool will be automatically determined given your environment. If you want to force to use podman instead of docker , supply the -b switch to the operator-sdk bundle validate command. Any warnings here might turn into failing pipeline tests here. Please correct all issues displayed. A list of fields that are scanned can also be reviewed with this list .","title":"Using operator-sdk to validate your Operator"},{"location":"testing-operators/#publishing-your-operator-metadata-to-a-catalog-for-testing","text":"","title":"Publishing your Operator metadata to a catalog for testing"},{"location":"testing-operators/#building-a-catalog-using-packagemanifest-format","text":"When your Operator metadata is formatted in packagemanifest layout you need to place it in the directory structure of the community-operators repository, according to pre-requisites step . For example, assuming version 2.0.0 is the version you like to test: $ tree upstream-community-operators/ upstream-community-operators/ \u2502 ... \u2502 \u2514\u2500\u2500my-operator/ \u251c\u2500\u2500 0.1.0 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v0.1.0.clusterserviceversion.yaml \u251c\u2500\u2500 0.5.0 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v0.5.0.clusterserviceversion.yaml \u251c\u2500\u2500 1.0.0 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v1.0.0.clusterserviceversion.yaml \u251c\u2500\u2500 2.0.0 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v2.0.0.clusterserviceversion.yaml \u2514\u2500\u2500 my-operator.package.yaml You can build a catalog for OLM containing either all Operators or just yours with a Dockerfile like this FROM quay.io/operator-framework/upstream-registry-builder as builder COPY upstream-community-operators manifests RUN /bin/initializer -o ./bundles.db FROM scratch COPY --from = builder /etc/nsswitch.conf /etc/nsswitch.conf COPY --from = builder /bundles.db /bundles.db COPY --from = builder /bin/registry-server /registry-server COPY --from = builder /bin/grpc_health_probe /bin/grpc_health_probe EXPOSE 50051 ENTRYPOINT [ \"/registry-server\" ] CMD [ \"--database\" , \"bundles.db\" ] Simply adjust the second line to either include all OpenShift Community Operators ( community-operators ), all OperatorHub.io Operators ( upstream-community-operators ) or just your Operator (e.g. upstream-community-operator/my-operator ). Place the Dockerfile in the top-level directory of your cloned copy of this repo, build it and push it to a registry from where you can download it to your Kubernetes cluster later. For example: podman build -f catalog.Dockerfile -t my-test-catalog:latest . podman tag my-test-catalog:latest quay.io/myaccount/my-test-catalog:latest podman push quay.io/myaccount/my-test-catalog:latest","title":"Building a catalog using packagemanifest format"},{"location":"testing-operators/#building-a-catalog-using-bundles","text":"When your Operator metadata is formatted in bundle layout you can optionally add it to the existing directory structure like described in the pre-requisites step . For building a catalog this is not required because with Operator bundles versions are incrementally added to an existing or empty catalog. For example, assuming version 2.0.0 is the version you like to test: $ tree my-operator/ my-operator/ \u251c\u2500\u2500 0.1.0 \u2502 \u251c\u2500\u2500 manifests \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2502 \u2514\u2500\u2500 my-operator.v0.1.0.clusterserviceversion.yaml \u2502 \u251c\u2500\u2500 metadata \u2502 \u2502 \u2514\u2500\u2500 annotations.yaml \u2502 \u2514\u2500\u2500 Dockerfile \u251c\u2500\u2500 0.5.0 \u2502 \u251c\u2500\u2500 manifests \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2502 \u2514\u2500\u2500 my-operator.v0.5.0.clusterserviceversion.yaml \u2502 \u251c\u2500\u2500 metadata \u2502 \u2502 \u2514\u2500\u2500 annotations.yaml \u2502 \u2514\u2500\u2500 Dockerfile \u251c\u2500\u2500 1.0.0 \u2502 \u251c\u2500\u2500 manifests \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2502 \u2514\u2500\u2500 my-operator.v1.0.0.clusterserviceversion.yaml \u2502 \u251c\u2500\u2500 metadata \u2502 \u2502 \u2514\u2500\u2500 annotations.yaml \u2502 \u2514\u2500\u2500 Dockerfile \u2514\u2500\u2500 2.0.0 \u251c\u2500\u2500 manifests \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v2.0.0.clusterserviceversion.yaml \u251c\u2500\u2500 metadata \u2502 \u2514\u2500\u2500 annotations.yaml \u2514\u2500\u2500 Dockerfile ... Simply build your bundle using the Dockerfile that is part of every Operator bundle. If you are new to this format please consult the operator-registry documentation. If you used operator-sdk to develop your Operator you can also leverage its packaging tooling to create a bundle . To build your bundle simply build the image and push it to a registry of your choice: podman build -f 2.0.0/Dockerfile -t my-operator:v2.0.0 2.0.0/ podman push my-operator:v2.0.0 quay.io/myaccount/my-operator:v2.0.0 With the bundle published to a registry you can now leverage opm to add it to the existing catalog of community operators: for OpenShift this is quay.io/openshift-community-operators/catalog:latest for OperatorHub.io this is quay.io/operatorhubio/catalog:latest opm will create a catalog image with your Operator added, like so: opm index add --bundles quay.io/myaccount/my-operator:v2.0.0 --from-index quay.io/operatorhubio/catalog:latest --tag quay.io/myaccount/my-test-catalog:latest You then push the resulting catalog image to a registry of your choice as well: podman push quay.io/myaccount/my-test-catalog:latest opm also supports multiple container tools via the -c switch. You can also omit the final step to build the catalog image and instead output the Dockerfile that would be used. Consult the help output for that via opm index add --help You now have a catalog image available for OLM to install your Operator version from.","title":"Building a catalog using bundles"},{"location":"testing-operators/#testing-operator-deployment-on-kubernetes","text":"","title":"Testing Operator Deployment on Kubernetes"},{"location":"testing-operators/#1-installing-operator-lifecycle-manager","text":"If you are not using OpenShift you first need to install the Operator Lifecycle Manager on your cluster. The following steps assume you already have a running Kubernetes cluster that is currently selected as your current-context via kubectl . If you, you can quickly spin up a cluster using tools like KIND or minikube mentioned in the pre-requisites section , e.g. kind create cluster You can install the Operator Lifecycle Manager using either operator-sdk or kubectl . Option 1: Install the Operator Lifecycle Manager using operator-sdk : operator-sdk olm install Verify that OLM is installed correctly: operator-sdk olm status This should output something like the following: INFO[0000] Fetching CRDs for version \"0.16.1\" INFO[0000] Fetching resources for version \"0.16.1\" INFO[0002] Successfully got OLM status for version \"0.16.1\" NAME NAMESPACE KIND STATUS operators.operators.coreos.com CustomResourceDefinition Installed operatorgroups.operators.coreos.com CustomResourceDefinition Installed installplans.operators.coreos.com CustomResourceDefinition Installed clusterserviceversions.operators.coreos.com CustomResourceDefinition Installed olm-operator olm Deployment Installed subscriptions.operators.coreos.com CustomResourceDefinition Installed olm-operator-binding-olm ClusterRoleBinding Installed operatorhubio-catalog olm CatalogSource Installed olm-operators olm OperatorGroup Installed aggregate-olm-view ClusterRole Installed catalog-operator olm Deployment Installed aggregate-olm-edit ClusterRole Installed olm Namespace Installed global-operators operators OperatorGroup Installed operators Namespace Installed packageserver olm ClusterServiceVersion Installed olm-operator-serviceaccount olm ServiceAccount Installed catalogsources.operators.coreos.com CustomResourceDefinition Installed system:controller:operator-lifecycle-manager ClusterRole Installed Option 2: Install the Operator Lifecycle Manager using kubectl : kubectl apply -f https://github.com/operator-framework/operator-lifecycle-manager/releases/download/v0.17.0/crds.yaml kubectl apply -f https://github.com/operator-framework/operator-lifecycle-manager/releases/download/v0.17.0/olm.yaml You can check if Operator Lifecycle Manager status via the following command: kubectl get pods -n olm This should output something like the following: NAME READY STATUS RESTARTS AGE catalog-operator-7b4788ffb5-9jggk 1/1 Running 0 23h olm-operator-678d76b95c-r492j 1/1 Running 0 23h operatorhubio-catalog-lqw72 1/1 Running 0 23h packageserver-7cfd786c67-pwpnn 1/1 Running 0 23h packageserver-7cfd786c67-tkwbm 1/1 Running 0 23h","title":"1. Installing Operator Lifecycle Manager"},{"location":"testing-operators/#2-adding-the-catalog-containing-your-operator","text":"Create a CatalogSource instance in the olm namespace to reference in the Operator catalog image that contains your Operator version to test: apiVersion : operators.coreos.com/v1alpha1 kind : CatalogSource metadata : name : my-test-catalog namespace : olm spec : sourceType : grpc image : quay.io/myaccount/my-test-catalog:latest Deploy the CatalogSource resource: kubectl apply -f catalog-source.yaml If you created your test catalog containing all existing community operators , you should delete the default catalog that OLM ships with to avoid a lot of duplicate entries: kubectl delete catalogsource operatorhubio-catalog -n olm Verify your custom catalog got loaded: $ kubectl get catalogsource -n olm NAME DISPLAY TYPE PUBLISHER AGE my-test-catalog grpc 3m32s [...] Verify the health of your catalog: kubectl get catalogsource my-test-catalog -n olm -o yaml","title":"2. Adding the catalog containing your Operator"},{"location":"testing-operators/#3-view-available-operators","text":"Inspect the list of loaded packagemanifests on the system with the following command to filter for your Operator kubectl get packagemanifests | grep my-operator The example should look like this: grep my-operator 1h2m If your Operator appears in this list, the catalog was successfully parsed and the Operator is now available to install.","title":"3. View Available Operators"},{"location":"testing-operators/#4-create-an-operatorgroup","text":"An OperatorGroup is used to denote which namespaces your Operator should be watching. It must exist in the namespace where your operator should be deployed, we'll use default in this example. Its configuration depends on whether your Operator supports watching its own namespace, a single other namespace or all namespaces (as indicated by spec.installModes in the CSV). Create the following file as operator-group.yaml if your Operator supports watching its own or a single namespace. If your Operator supports watching all namespaces you can skip this step and proceed to creating the Subscription object in the operators namespace. An OperatorGroup already exists there with spec.targetNamespace empty. This kind of OperatorGroup instructs the Operator to watch all namespaces. apiVersion : operators.coreos.com/v1alpha2 kind : OperatorGroup metadata : name : my-operatorgroup namespace : default spec : targetNamespaces : - default Deploy the OperatorGroup resource: kubectl apply -f operator-group.yaml","title":"4. Create an OperatorGroup"},{"location":"testing-operators/#5-create-a-subscription","text":"The last step is to ask OLM to install your Operator. A Subscription is created to represent the intent to install an Operator and keep it updated (automatically even) with a newer version from the catalog. This requires you to tell OLM which Operator, in which version from which channel you want to install and where the catalog is, that contains the Operator. Save the following to a file named: operator-subscription.yaml : apiVersion : operators.coreos.com/v1alpha1 kind : Subscription metadata : name : my-operator-subscription namespace : default spec : channel : <channel-name> name : my-operator startingCSV : my-operator.v2.0.0 source : my-test-catalog sourceNamespace : olm If your Operator supports watching all namespaces, change the Subscription namespace from default to operators . (This namespace already has an OperatorGroup ). In any case, replace <channel-name> with the contents of channel.name in your package.yaml file if you have the packagemanifest format or with the contents from operators.operatorframework.io.bundle.channels.v1 in annotations.yaml if you have the bundle format. Then create the Subscription : kubectl apply -f operator-subscription.yaml","title":"5. Create a Subscription"},{"location":"testing-operators/#6-verify-operator-health","text":"Watch your Operator being deployed by OLM from the catalog with the following command. Change the namespace from default to operators if you installed your Subscrption there. Use the watch switch -w : $ kubectl get clusterserviceversion -n default -w NAME DISPLAY VERSION REPLACES PHASE my-operator.v2.0.0 My Operator 2.0.0 my-operator.v1.0.0 my-operator.v2.0.0 My Operator 2.0.0 my-operator.v1.0.0 my-operator.v2.0.0 My Operator 2.0.0 my-operator.v1.0.0 my-operator.v2.0.0 My Operator 2.0.0 my-operator.v1.0.0 Pending my-operator.v2.0.0 My Operator 2.0.0 my-operator.v1.0.0 InstallReady my-operator.v2.0.0 My Operator 2.0.0 my-operator.v1.0.0 Installing my-operator.v2.0.0 My Operator 2.0.0 my-operator.v1.0.0 Installing my-operator.v2.0.0 My Operator 2.0.0 my-operator.v1.0.0 Installing my-operator.v2.0.0 My Operator 2.0.0 my-operator.v1.0.0 Installing my-operator.v2.0.0 My Operator 2.0.0 my-operator.v1.0.0 Succeeded The ClusterServiceVersion object represents your installed Operator per version. It can take a couple of seconds to be created as part of the Subscription request. There will always be a ClusterServiceVersion in the namespace of the Subscription . If the OperatorGroup was configured to \"watch\" a list of all namespaces, the ClusterServiceVersion object will be copied to all those namespaces. This might take additional time (usually around 30 secs). If the above command succeeds and the ClusterServiceVersion has transitioned to the Succeeded phase you will also find your Operator's Deployment(s) in the same namespace where the Subscription is. This is your Operator running: kubectl get deployment -n default","title":"6. Verify Operator health"},{"location":"testing-operators/#testing-operator-deployment-on-openshift","text":"The process to test on OpenShift Container Platform and OKD 4.3 or newer is exactly the same as described above with the exception that OLM is already installed. You can use the same CLI steps to test your Operator but it can however also be done via the UI.","title":"Testing Operator Deployment on OpenShift"},{"location":"testing-operators/#1-create-the-catalogsource","text":"Create a CatalogSource instance in the openshift-marketplace namespace to reference the Operator catalog image that contains your Operator version to test: apiVersion : operators.coreos.com/v1alpha1 kind : CatalogSource metadata : name : my-test-catalog namespace : openshift-marketplace spec : sourceType : grpc image : quay.io/myaccount/my-test-catalog:latest You can use the OpenShift Console YAML editor for that or deploy the CatalogSource resource on the CLI: oc apply -f catalog-source.yaml If you created your test catalog containing all existing community operators , you should delete the default catalog that OLM ships with to avoid a lot of duplicate entries. To do this navigate to Administration on the main navigation bar on the left and select the Cluster Settings item. In this view, select the Global Configuration tab and filter for OperatorHub : Click on the OperatorHub item and select the YAML tab. In the YAML editor set the disabled property for the community-catalog to true : Click Save .","title":"1. Create the CatalogSource"},{"location":"testing-operators/#2-find-your-operator-in-the-operatorhub-ui","text":"Go to your OpenShift UI and find your Operator by filtering for the Custom category:","title":"2. Find your Operator in the OperatorHub UI"},{"location":"testing-operators/#3-install-your-operator-from-operatorhub","text":"To install your Operator simply click its icon and in the proceeding dialog click Install . You will be asked where to install your Operator. Select either of the desired installation modes, if your Operator supports it and then click Subscribe You will be forwarded to the Subscription Management section of the OLM UI and after a couple of moments, your Operator will be transitioning to Installed .","title":"3. Install your Operator from OperatorHub"},{"location":"testing-operators/#4-verify-operator-health","text":"Change to the Installed Operators section in the left-hand navigation menu to verify your Operator's installation status: It should have transitioned into the state InstallationSucceeded . You can now test it by starting to use its APIs.","title":"4. Verify Operator health"},{"location":"testing-operators/#testing-with-scorecard","text":"If your Operator is up and running you can verify it is working as intended using its APIs. Additionally, you can run operator-sdk 's scorecard utility for validating against good practice and correctness of your Operator. Assuming you are still in your top-level directory where my-operator/ is your bundle location follow these instructions to test your Operator using the Operator-SDK : Running your Operator with scorecard","title":"Testing with scorecard"},{"location":"testing-operators/#additional-resources","text":"Cluster Service Version Spec Example Bundle","title":"Additional Resources"},{"location":"tests-in-pr/","text":"PR Continuous Integration Operators submitted to this repo are automatically tested on a Kubernetes cluster before being merged. The Kubernetes distribution used for testing depends on which directory the operator is submitted to. Ideally, all tests should pass before merging. You can test operators locally using following documentation. CI test scripts Test scripts are written in Ansible and located in our upstream-community branch . There are 3 test types. A list of tests is shown in the following table. Test type Description kiwi Full operator test lemon Full test of operator to be deployed from scratch orange Full test of operator to be deployed with existing bundles in quay registry all kiwi,lemon,orange [kiwi] - Full operator test Full operator tests - Building bundle image - from packagemanifest format - from bundle format - Sanity check of operator version (when multiple only last test is done) - Validation using operator-sdk validate - Building a temporary catalog with one operator version in it - Deployment of an operator on kind (k8s) cluster (only for kuberbetes-operator) [lemon] - Test of operator to be deployed from scratch Test if deployment is possible from the scratch. It means creating bundle images and index image. Build all bundle images Build catalog [orange] - Test of operator to be deployed with existing bundles in quay registry Test if an operator can be added to index from existing bundles from production (quay.io) Build a current operator version locally Use older versions from quay.io Build catalog OLM Deployment with the OLM involves creating several required manifest files to create CustomResourceDefinitions (CRD's) and the operators' Deployment using its ClusterServiceVersion (CSV) in-cluster. test-operator will create a operator-registry Docker image containing the operators' bundled manifests, and CatalogSource and Subscription manifest that allows the OLM to find the registry image and deploy a particular CSV from the registry, respectively. Failure to successfully deploy an operator using the OLM results in a test failure, as all operators are expected to be deployable in this manner. Scorecard The Operator SDK scorecard suggests modifications applicable to an operator based on development best practices. The scorecard runs static checks on operator manifests and runtime tests to ensure an operator is using cluster resources correctly. A Custom Resource (CR) is created by the scorecard for use in runtime tests, so alm-examples must be populated. The scorecard utility runs through multiple test scenarios, some of which are required and others are optional. Currently the tests are configured like this. Mandatory tests that need to pass for the PR to be accepted: checkspectest - verifies that the CRs have a spec section writingintocrshaseffecttest - verifies that writing into the CR causes the Operator to issue requests against the Kubernetes API server Recommended tests that should pass in order to have a well-behaved operator: checkstatustest - verifies whether the CRs status block gets updated by the Operator to indicate reconciliation. See the scorecard test documentation for more information. test-operator injects a scorecard proxy container and volume into an operator's CSV manifest before deployment; this is necessary to get API server logs, from which the scorecard determines runtime test results. These modifications are not persistent, as they're only needed for testing. Note : no explicit number of points or percentage is necessary to achieve before merging yet . These are suggestions to improve your operator. Upstream operators Operators submitted to the upstream-community-operators/ directory are tested against a KIND instance deployed on a [Travis CI][travis-ci] environment. The OLM is installed locally in this case. OpenShift operators Operators submitted to the community-operators/ directory are tested against an OpenShift 4.0 cluster deployed on AWS using the ci-operator .","title":"Tests in PR explained"},{"location":"tests-in-pr/#pr-continuous-integration","text":"Operators submitted to this repo are automatically tested on a Kubernetes cluster before being merged. The Kubernetes distribution used for testing depends on which directory the operator is submitted to. Ideally, all tests should pass before merging. You can test operators locally using following documentation.","title":"PR Continuous Integration"},{"location":"tests-in-pr/#ci-test-scripts","text":"Test scripts are written in Ansible and located in our upstream-community branch . There are 3 test types. A list of tests is shown in the following table. Test type Description kiwi Full operator test lemon Full test of operator to be deployed from scratch orange Full test of operator to be deployed with existing bundles in quay registry all kiwi,lemon,orange","title":"CI test scripts"},{"location":"tests-in-pr/#upstream-operators","text":"Operators submitted to the upstream-community-operators/ directory are tested against a KIND instance deployed on a [Travis CI][travis-ci] environment. The OLM is installed locally in this case.","title":"Upstream operators"},{"location":"tests-in-pr/#openshift-operators","text":"Operators submitted to the community-operators/ directory are tested against an OpenShift 4.0 cluster deployed on AWS using the ci-operator .","title":"OpenShift operators"},{"location":"troubleshooting/","text":"Troubleshooting PR-traffic-light failures: Operator changes detected with ci changes and 'allow/ci-changes' is not set Do not modify files outside of community-operators and upstream-community-operators, please rebase or fix and push changes. Changes in both 'community-operators' and 'upstream-community-operators' dirs Every operator must have separate PR. Separate for Kubernetes (upstream) and separate for Openshift (community). It helps more precise testing and also granular revert if needed. Sometimes just rebase is needed. Multiple operators are changed Multiple operators in the same stream are updated. This is not allowed. It could be a result of not rebased PR. Please rebase or create a separate PR for every operator. We support only a single file modification in the case of 'ci.yaml' file. If you want to update it, please make an extra PR with 'ci.yaml' file modification only!!! Please make an extra PR and modify only ci.yaml . Package metadata test (kiwi) test failures: Operator deployment with OLM failed Could be multiple reasons, but first of all, please check if your operator image can be pulled from a public location. This is the most common root cause for operators failing to start. Tests for this situation are planned in backlog, so the pipeline will tell you this in near future automatically. csv.Spec.Icon not specified Icon is mandatory, more information here . ci/prow/deploy-operator-on-openshift failures: Test Failed? To investigate every failed Openshift test, please open test Details and click Show all hidden lines . Then scroll to the bottom and check all logs from bottom to top. Logs are ordered according to debugging value, the most important logs are at the bottom, the least important and some initialization staff is at the top of every run. Temp index not found. Are your commits squashed? Error message combined with Missing '$OP_NAME' or similar. The majority of tests ending without a temporary index are caused by PR containing too many commits. Please (rebase) squash and force-push. If it is not your case, you can inspect logs from building temporary indexes for test purposes here https://github.com/operator-framework/community-operators/actions?query=workflow%3Aprepare-test-index. Operation cannot be fulfilled on ... the object has been modified; please apply your changes to the latest version and try again This is not an issue, you can ignore it. ImagePullBackOff error message Unable to pull your operator image. Operator deploy tests (lemon/oragne) test failures: All operator versions are already in the catalog You are trying to edit an existing operator version. It is not recommended. But there are some exceptions, where you just edit some description or link. In this case, repository maintainers can set appropriate labels to override such errors and approve release pipeline action to overwrite an existing operator.","title":"Troubleshooting"},{"location":"troubleshooting/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"troubleshooting/#pr-traffic-light-failures","text":"Operator changes detected with ci changes and 'allow/ci-changes' is not set Do not modify files outside of community-operators and upstream-community-operators, please rebase or fix and push changes. Changes in both 'community-operators' and 'upstream-community-operators' dirs Every operator must have separate PR. Separate for Kubernetes (upstream) and separate for Openshift (community). It helps more precise testing and also granular revert if needed. Sometimes just rebase is needed. Multiple operators are changed Multiple operators in the same stream are updated. This is not allowed. It could be a result of not rebased PR. Please rebase or create a separate PR for every operator. We support only a single file modification in the case of 'ci.yaml' file. If you want to update it, please make an extra PR with 'ci.yaml' file modification only!!! Please make an extra PR and modify only ci.yaml .","title":"PR-traffic-light failures:"},{"location":"troubleshooting/#package-metadata-test-kiwi-test-failures","text":"Operator deployment with OLM failed Could be multiple reasons, but first of all, please check if your operator image can be pulled from a public location. This is the most common root cause for operators failing to start. Tests for this situation are planned in backlog, so the pipeline will tell you this in near future automatically. csv.Spec.Icon not specified Icon is mandatory, more information here .","title":"Package metadata test (kiwi) test failures:"},{"location":"troubleshooting/#ciprowdeploy-operator-on-openshift-failures","text":"Test Failed? To investigate every failed Openshift test, please open test Details and click Show all hidden lines . Then scroll to the bottom and check all logs from bottom to top. Logs are ordered according to debugging value, the most important logs are at the bottom, the least important and some initialization staff is at the top of every run. Temp index not found. Are your commits squashed? Error message combined with Missing '$OP_NAME' or similar. The majority of tests ending without a temporary index are caused by PR containing too many commits. Please (rebase) squash and force-push. If it is not your case, you can inspect logs from building temporary indexes for test purposes here https://github.com/operator-framework/community-operators/actions?query=workflow%3Aprepare-test-index. Operation cannot be fulfilled on ... the object has been modified; please apply your changes to the latest version and try again This is not an issue, you can ignore it. ImagePullBackOff error message Unable to pull your operator image.","title":"ci/prow/deploy-operator-on-openshift failures:"},{"location":"troubleshooting/#operator-deploy-tests-lemonoragne-test-failures","text":"All operator versions are already in the catalog You are trying to edit an existing operator version. It is not recommended. But there are some exceptions, where you just edit some description or link. In this case, repository maintainers can set appropriate labels to override such errors and approve release pipeline action to overwrite an existing operator.","title":"Operator deploy tests (lemon/oragne) test failures:"},{"location":"legacy/using-obsolete-scripts/","text":"Automate testing your Operator locally For convenience, in addition to the manual test instructions we provide a Makefile based test automation. This will automate all the manual steps referred to in Testing Operator Deployment on Kubernetes . In addition the scorecard test from the Operator-SDK will be executed. This is currently tested on Kubernetes in Docker but should work on other Kubernetes systems as well. Prerequisites You need the following installed on your local machine: Linux or macOS host Docker make KIND (if no existing Kubernetes cluster is available via KUBECONFIG or in ~/.kube/config ) Important: Notice, that this script uses a container to execute the test. Your KUBECONFIG will be bind mounted into the container. Therefore no config-helpers or references to files on your host machine are allowed. This is usually the case for minikube or GKE clusters. All further dependencies are encapsulated in a container image that this Makefile will execute as a test driver. Available test modes The Makefile supports two test modes. Both have these supported options: Options: OP_PATH - relative path to your operator (required) OP_VER - version of operator (if not provided the latest will be determined from your package.yaml ) OP_CHANNEL - channel of operator if is not provided it will be parsed by operator package yaml or use the default ones VERBOSE - enable verbose output of executed subcommands Deploying and Testing your Operator Using the Operator Lifecycle Manager (OLM) your Operator will be packaged into a temporary catalog, containing all currently published community operators and yours. OLM will be installed for you if not present. Using the current community catalog as a base allows you to test with dependencies on Operators currently published in this catalog. If you have dependencies outside of this catalog, you need to prepare your own cluster, install OLM, and ship a catalog with these dependencies present; otherwise installation will fail. You can provide a Kubernetes cluster as a testbed via KUBECONFIG or ~/.kube/confg . If you have multiple cluster contexts configured in your KUBECONFIG you will be able to select one. If you have no cluster configured or reachable the Makefile will install a kind cluster named operator-test for you. For this type of test, additionally the following options exist: NO_KIND - if set to 1 no attempt to bring up a kind cluster will be made. In this case you need to specify CATALOG_IMAGE CATALOG_IMAGE - when NO_KIND is set to 1 you need to specify a container registry image location you have push privileges for and from which the image can be pulled again later by OLM without authentication. This parameter is ignored when NO_KIND is absent or set to 0 since the catalog image can be loaded directly into a KIND cluster. CLEAN_MODE - any of NORMAL , NONE and FORCE . As the test installs OLM components in your Kubernetes cluster this controls the clean up of those. In NORMAL clean up will happen if no errors occured. When set to NONE clean up is omitted. When set to FORCE clean up will always be done. Default is NORMAL . INSTALL_MODE - any of OwnNamespace , SingleNamespace , AllNamespaces . this controls the installation mode of the Operator and should be set according to what your Operator states as supported in the installModes section of the CSV. Default is SingleNamepsace . You can start by just deploying your Operator: make operator.install OP_PATH=upstream-community-operators/cockroachdb Pulling docker image [ Processing ] Pulling docker image [ OK ] Find kube config [ /home/dmesser/.kube/config ] Find kube cluster [ Not found ] Start KIND [ Processing ] Start KIND [ OK ] Building catalog image [ Processing ] Building catalog image [ OK ] Operator version detected [ 1.7.2 ] Creating namespace [ Processing ] Creating namespace [ OK ] Verify operator [ Processing ] Verify operator [ OK ] Install OLM [ Processing ] Install OLM [ OK ] Building manifests [ Processing ] Building manifests [ OK ] Operator Deployment [ Processing ] Applying object to cluster [ Processing ] Applying object to cluster [ OK ] Checking if subscriptions passes [ Processing ] Checking if subscriptions passes [ OK ] Checking if CSV passes [ Processing ] Checking if CSV passes [ OK ] Operator Deployment [ OK ] This way you can test if your Operator is packaged correctly. You can also run a test that will deploy your Operator and checks if it behaves correctly according to scorecard (which is part of the Operator-SDK). scorecard will use the example CRs defined in metadata.annotations.alm-examples in the CSV to try to use your Operator and observe its behavior. Example, run from the top-level directory of this repository: [...] make operator.test OP_PATH=upstream-community-operators/cockroachdb [...] Instrumenting Operator for test [ Processing ] creating CR files [ Processing ] creating CR files [ OK ] injecting scorecard proxy [ Processing ] injecting scorecard proxy [ OK ] Instrumenting Operator for test [ OK ] Running scorecard trough all supplied CRs [ Processing ] Running required tests [ Processing ] Running required tests [ OK ] Running recommended tests [ Processing ] Running recommended tests [ OK ] Running required tests [ Processing ] Running required tests [ OK ] Running recommended tests [ Processing ] Running recommended tests [ OK ] Running scorecard trough all supplied CRs [ OK ] Cleaning up Operator resources [ Processing ] Cleaning up Operator resources [ OK ] Cleaning up Operator definition [ Processing ] Cleaning up Operator definition [ OK ] Cleaning up namespace [ Processing ] Cleaning up namespace [ OK ] Troubleshooting Here are some common scenarios, why your test can fail: Failures when linting Operator metadata ERROR: metadata.annotations.alm-examples contains invalid json string [1.4.4/my-operator.v1.4.4.clusterserviceversion.yaml] The linter checks for valid JSON in metadata.annotations.alm-examples . The rest of the CSV is supposed to be YAML. Failures when loading the Operator into the Community Catalog my-operator.v2.1.11 specifies replacement that couldn't be found Explanation: This happens because the catalog cannot load your Operator since it's pointing to a non-existent previous version of your Operator using spec.replaces . For updates, it is important that this property points to another, older version of your Operator that is already in the catalog. error adding operator bundle : error decoding CRD: no kind \\\"CustomResourceDefinition\\\" is registered for version \\\"apiextensions.k8s.io/v1\\\" in scheme \\\"pkg/registry/bundle.go Explanation: Currently OLM does not yet support handling CRDs using apiextensions.k8s.io/v1 . This will improve soon. Until then you need to resort back to apiextensions.k8s.io/v1beta . error loading manifests from directory: error checking provided apis in bundle : couldn't find charts.someapi.k8s.io/v1alpha1/myapi (my-custom-resource) in bundle. found: map[] Explanation: Your Operator claims ownership of a CRD that it does not ship. Check for spelling of Group/Version/Kind in spec.customresourcedefinitions.owned in the CSV. error loading package into db: [FOREIGN KEY constraint failed, no default channel specified for my-operator] Explanation: This happens when either - Your Operator package defines more than one channel in package.yaml but does not define defaultChannel . - The package just defines a single channel (in which case you can omit defaultChannel ) but the catalog couldn't load the CSV that this channel points to using currentCSV . This can happen when in the CSV the specified name in metadata.name is actually different from what currentCSV points to. Failures when deploying via OLM Check if subscription passes times out Explanation: In this case the Subscription object created by the test suite did not transition to the state AtLatestKnown before hitting a timeout. There are various reasons for this, ranging from the catalog pod crashing to problems with the catalog-operator pod of OLM itself. In any case, the logs of either pod will likely help troubleshooting and finding the root cause. Check if CSV passes times out Explanation: OLM could not install the Operator's Deployment from its CSV before hitting a timeout. This is usually due to Deployment reaching its expected replica count, likely because the pod is crash-looping. Failures during tests of the Operator with Operator-SDK scorecard failed to get proxyPod: timed out waiting for the condition: Explanation: If this happened it is likely the Operator pod crashed in the middle of the scorecard test suite. For example, when it failed to parse a Custom Resource fed to scorecard from the list in metadata.annotations.alm-examples . OLM will wait for the Deployment of the Operator to recover before re-installing the Operator. Re-installation changes the Operator pod's name and hence scorecard fails to reach the logs of scorecard proxy using its old name. failed to create cr resource: object is being deleted: someapi.k8s.io \"myCRD\" already exists: Explanation: This can happen when your Operator automatically creates a CR on startup, with the same name of an example for that CR provided in the CSV metadata.annotations.alm-examples section. Simply use a different name in the example. Otherwise, your Operator could be slow to delete a CR due to a finalizer. Additional shortcuts Clean up after a failed test As explained above, the default CLEAN_MODE of NORMAL will delete anything that got installed on your cluster if all tests run correctly.. If something fails, the deployed resources will not be deleted in order to give you a chance to debug. After you have finished debugging you can use the following command to clean up any residual resources as part of a test of a particular Operator: make operator.cleanup OP_PATH=upstream-community-operators/cockroachdb Install a KIND cluster Install a kind cluster as a testbed for the Operator deployment. $ kind create cluster --name operator-test This command will create a Kubernetes in Docker cluster: $ kind get clusters operator-test $ kind get nodes --name operator-test operator-test-control-plane Install Operator Lifecycle Manager Install OLM to an existing cluster (determined via KUBECONFIG or ~/.kube/config ). make olm.install","title":"Automate testing your Operator locally"},{"location":"legacy/using-obsolete-scripts/#automate-testing-your-operator-locally","text":"For convenience, in addition to the manual test instructions we provide a Makefile based test automation. This will automate all the manual steps referred to in Testing Operator Deployment on Kubernetes . In addition the scorecard test from the Operator-SDK will be executed. This is currently tested on Kubernetes in Docker but should work on other Kubernetes systems as well.","title":"Automate testing your Operator locally"},{"location":"legacy/using-obsolete-scripts/#prerequisites","text":"You need the following installed on your local machine: Linux or macOS host Docker make KIND (if no existing Kubernetes cluster is available via KUBECONFIG or in ~/.kube/config ) Important: Notice, that this script uses a container to execute the test. Your KUBECONFIG will be bind mounted into the container. Therefore no config-helpers or references to files on your host machine are allowed. This is usually the case for minikube or GKE clusters. All further dependencies are encapsulated in a container image that this Makefile will execute as a test driver.","title":"Prerequisites"},{"location":"legacy/using-obsolete-scripts/#available-test-modes","text":"The Makefile supports two test modes. Both have these supported options:","title":"Available test modes"},{"location":"legacy/using-obsolete-scripts/#options","text":"OP_PATH - relative path to your operator (required) OP_VER - version of operator (if not provided the latest will be determined from your package.yaml ) OP_CHANNEL - channel of operator if is not provided it will be parsed by operator package yaml or use the default ones VERBOSE - enable verbose output of executed subcommands","title":"Options:"},{"location":"legacy/using-obsolete-scripts/#deploying-and-testing-your-operator","text":"Using the Operator Lifecycle Manager (OLM) your Operator will be packaged into a temporary catalog, containing all currently published community operators and yours. OLM will be installed for you if not present. Using the current community catalog as a base allows you to test with dependencies on Operators currently published in this catalog. If you have dependencies outside of this catalog, you need to prepare your own cluster, install OLM, and ship a catalog with these dependencies present; otherwise installation will fail. You can provide a Kubernetes cluster as a testbed via KUBECONFIG or ~/.kube/confg . If you have multiple cluster contexts configured in your KUBECONFIG you will be able to select one. If you have no cluster configured or reachable the Makefile will install a kind cluster named operator-test for you. For this type of test, additionally the following options exist: NO_KIND - if set to 1 no attempt to bring up a kind cluster will be made. In this case you need to specify CATALOG_IMAGE CATALOG_IMAGE - when NO_KIND is set to 1 you need to specify a container registry image location you have push privileges for and from which the image can be pulled again later by OLM without authentication. This parameter is ignored when NO_KIND is absent or set to 0 since the catalog image can be loaded directly into a KIND cluster. CLEAN_MODE - any of NORMAL , NONE and FORCE . As the test installs OLM components in your Kubernetes cluster this controls the clean up of those. In NORMAL clean up will happen if no errors occured. When set to NONE clean up is omitted. When set to FORCE clean up will always be done. Default is NORMAL . INSTALL_MODE - any of OwnNamespace , SingleNamespace , AllNamespaces . this controls the installation mode of the Operator and should be set according to what your Operator states as supported in the installModes section of the CSV. Default is SingleNamepsace . You can start by just deploying your Operator: make operator.install OP_PATH=upstream-community-operators/cockroachdb Pulling docker image [ Processing ] Pulling docker image [ OK ] Find kube config [ /home/dmesser/.kube/config ] Find kube cluster [ Not found ] Start KIND [ Processing ] Start KIND [ OK ] Building catalog image [ Processing ] Building catalog image [ OK ] Operator version detected [ 1.7.2 ] Creating namespace [ Processing ] Creating namespace [ OK ] Verify operator [ Processing ] Verify operator [ OK ] Install OLM [ Processing ] Install OLM [ OK ] Building manifests [ Processing ] Building manifests [ OK ] Operator Deployment [ Processing ] Applying object to cluster [ Processing ] Applying object to cluster [ OK ] Checking if subscriptions passes [ Processing ] Checking if subscriptions passes [ OK ] Checking if CSV passes [ Processing ] Checking if CSV passes [ OK ] Operator Deployment [ OK ] This way you can test if your Operator is packaged correctly. You can also run a test that will deploy your Operator and checks if it behaves correctly according to scorecard (which is part of the Operator-SDK). scorecard will use the example CRs defined in metadata.annotations.alm-examples in the CSV to try to use your Operator and observe its behavior. Example, run from the top-level directory of this repository: [...] make operator.test OP_PATH=upstream-community-operators/cockroachdb [...] Instrumenting Operator for test [ Processing ] creating CR files [ Processing ] creating CR files [ OK ] injecting scorecard proxy [ Processing ] injecting scorecard proxy [ OK ] Instrumenting Operator for test [ OK ] Running scorecard trough all supplied CRs [ Processing ] Running required tests [ Processing ] Running required tests [ OK ] Running recommended tests [ Processing ] Running recommended tests [ OK ] Running required tests [ Processing ] Running required tests [ OK ] Running recommended tests [ Processing ] Running recommended tests [ OK ] Running scorecard trough all supplied CRs [ OK ] Cleaning up Operator resources [ Processing ] Cleaning up Operator resources [ OK ] Cleaning up Operator definition [ Processing ] Cleaning up Operator definition [ OK ] Cleaning up namespace [ Processing ] Cleaning up namespace [ OK ]","title":"Deploying and Testing your Operator"},{"location":"legacy/using-obsolete-scripts/#troubleshooting","text":"Here are some common scenarios, why your test can fail:","title":"Troubleshooting"},{"location":"legacy/using-obsolete-scripts/#failures-when-linting-operator-metadata","text":"ERROR: metadata.annotations.alm-examples contains invalid json string [1.4.4/my-operator.v1.4.4.clusterserviceversion.yaml] The linter checks for valid JSON in metadata.annotations.alm-examples . The rest of the CSV is supposed to be YAML.","title":"Failures when linting Operator metadata"},{"location":"legacy/using-obsolete-scripts/#failures-when-loading-the-operator-into-the-community-catalog","text":"my-operator.v2.1.11 specifies replacement that couldn't be found Explanation: This happens because the catalog cannot load your Operator since it's pointing to a non-existent previous version of your Operator using spec.replaces . For updates, it is important that this property points to another, older version of your Operator that is already in the catalog. error adding operator bundle : error decoding CRD: no kind \\\"CustomResourceDefinition\\\" is registered for version \\\"apiextensions.k8s.io/v1\\\" in scheme \\\"pkg/registry/bundle.go Explanation: Currently OLM does not yet support handling CRDs using apiextensions.k8s.io/v1 . This will improve soon. Until then you need to resort back to apiextensions.k8s.io/v1beta . error loading manifests from directory: error checking provided apis in bundle : couldn't find charts.someapi.k8s.io/v1alpha1/myapi (my-custom-resource) in bundle. found: map[] Explanation: Your Operator claims ownership of a CRD that it does not ship. Check for spelling of Group/Version/Kind in spec.customresourcedefinitions.owned in the CSV. error loading package into db: [FOREIGN KEY constraint failed, no default channel specified for my-operator] Explanation: This happens when either - Your Operator package defines more than one channel in package.yaml but does not define defaultChannel . - The package just defines a single channel (in which case you can omit defaultChannel ) but the catalog couldn't load the CSV that this channel points to using currentCSV . This can happen when in the CSV the specified name in metadata.name is actually different from what currentCSV points to.","title":"Failures when loading the Operator into the Community Catalog"},{"location":"legacy/using-obsolete-scripts/#failures-when-deploying-via-olm","text":"Check if subscription passes times out Explanation: In this case the Subscription object created by the test suite did not transition to the state AtLatestKnown before hitting a timeout. There are various reasons for this, ranging from the catalog pod crashing to problems with the catalog-operator pod of OLM itself. In any case, the logs of either pod will likely help troubleshooting and finding the root cause. Check if CSV passes times out Explanation: OLM could not install the Operator's Deployment from its CSV before hitting a timeout. This is usually due to Deployment reaching its expected replica count, likely because the pod is crash-looping.","title":"Failures when deploying via OLM"},{"location":"legacy/using-obsolete-scripts/#failures-during-tests-of-the-operator-with-operator-sdk-scorecard","text":"failed to get proxyPod: timed out waiting for the condition: Explanation: If this happened it is likely the Operator pod crashed in the middle of the scorecard test suite. For example, when it failed to parse a Custom Resource fed to scorecard from the list in metadata.annotations.alm-examples . OLM will wait for the Deployment of the Operator to recover before re-installing the Operator. Re-installation changes the Operator pod's name and hence scorecard fails to reach the logs of scorecard proxy using its old name. failed to create cr resource: object is being deleted: someapi.k8s.io \"myCRD\" already exists: Explanation: This can happen when your Operator automatically creates a CR on startup, with the same name of an example for that CR provided in the CSV metadata.annotations.alm-examples section. Simply use a different name in the example. Otherwise, your Operator could be slow to delete a CR due to a finalizer.","title":"Failures during tests of the Operator with Operator-SDK scorecard"},{"location":"legacy/using-obsolete-scripts/#additional-shortcuts","text":"","title":"Additional shortcuts"},{"location":"legacy/using-obsolete-scripts/#clean-up-after-a-failed-test","text":"As explained above, the default CLEAN_MODE of NORMAL will delete anything that got installed on your cluster if all tests run correctly.. If something fails, the deployed resources will not be deleted in order to give you a chance to debug. After you have finished debugging you can use the following command to clean up any residual resources as part of a test of a particular Operator: make operator.cleanup OP_PATH=upstream-community-operators/cockroachdb","title":"Clean up after a failed test"},{"location":"legacy/using-obsolete-scripts/#install-a-kind-cluster","text":"Install a kind cluster as a testbed for the Operator deployment. $ kind create cluster --name operator-test This command will create a Kubernetes in Docker cluster: $ kind get clusters operator-test $ kind get nodes --name operator-test operator-test-control-plane","title":"Install a KIND cluster"},{"location":"legacy/using-obsolete-scripts/#install-operator-lifecycle-manager","text":"Install OLM to an existing cluster (determined via KUBECONFIG or ~/.kube/config ). make olm.install","title":"Install Operator Lifecycle Manager"}]}